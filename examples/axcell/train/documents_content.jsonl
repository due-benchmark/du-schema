{"name": "1703.10295v3", "contents": [{"tool_name": "djvu", "text": "a r X i v : 1 7 0 3 . 1 0 2 9 5 v 3 [ c s . C V ] 2 1 J u l 2 0 1 7 DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling Lachlan Tychsen-Smith, Lars Petersson CSIRO (Data61) 7 London Circuit, Canberra, ACT, 2601 Lachlan.Tychsen-Smith@data61.csiro.au, Lars.Petersson@data61.csiro.au Abstract We define the object detection from imagery problem as estimating a very large but extremely sparse bounding box dependent probability distribution. Subsequently we identify a sparse distribution estimation scheme, Directed Sparse Sampling, and employ it in a single end-to-end CNN based detection model. This methodology extends and for- malizes previous state-of-the-art detection models with an additional emphasis on high evaluation rates and reduced manual engineering. We introduce two novelties, a cor- ner based region-of-interest estimator and a deconvolution based CNN model. The resulting model is scene adaptive, does not require manually defined reference bounding boxes and produces highly competitive results on MSCOCO, Pas- cal VOC 2007 and Pascal VOC 2012 with real-time eval- uation rates. Further analysis suggests our model per- forms particularly well when finegrained object localiza- tion is desirable. We argue that this advantage stems from the significantly larger set of available regions-of-interest relative to other methods. Source-code is available from: https://github.com/lachlants/denet 1. Introduction Feed-forward neural networks exhibit good convergence properties given a random initialization under stochastic gradient descent (SGD) and, given an appropriate network design and training regime, can generalize well to previ- ously unseen data [8]. In particular, convolutional neural networks (CNNs) built from interleaved convolution and pooling layers with ReLU activation functions have set nu- merous benchmarks in computer vision tasks [8] [6] [20]. A number of methodologies have been developed to map their state-of-the-art dense regression and classification ca- pabilities to the problem of identifying axis aligned bound- ing boxes of object instances in images. In particular we highlight the relatively slow region based CNN approaches (R-CNN [4], Faster R-CNN [15]) and the more recent work on real-time detection (YOLO [14], SSD [12]). Input Image Base CNN Sparse Sample Classify CNN BBox Update Pr(s|BS ) Corner Detect Pr(t|k, y , x) Sampling Bounding Boxes Figure 1. A high level flow diagram depicting the DeNet method- ology. The CNN\u2019s are highlighted in blue, the novel components in purple and the outputs in yellow. The sampling bounding box dependency BS (highlighted in red) is held constant during back propagation to produce an end-to-end trained model. The cor- ner distribution and final classification distribution are jointly op- timized using cross entropy loss. Rather than focusing on obtaining state-of-the-art accu- racy in a competition environment (i.e . computationally un- constrained) in this paper we emphasis the dual task of ob- taining the best detection performance at a predefined eval- uation rate i.e. 60 Hz and 30 Hz. The primary contributions made within this paper include: \u2022 An improved theoretical understanding of modern de- tection methods and a generic framework in which to describe them i.e . Directed Sparse Sampling. \u2022 A novel, fast, region-of-interest estimator which doesn\u2019t require manually defined reference bounding boxes. \u2022 A novel application of deconvolution layers which greatly improves evaluation rates. \u2022 Six implementations of our method demonstrating competitive detection performance on a range of benchmarks. \u2022 An easily extended Theano based code release to facil- itate the research community. 1 1.1 . Related work In region based CNN detection (R-CNN) [4] the image is first preprocessed with a region proposal algorithm e.g . selective search [21], region proposal network (RPN) [15], etc. This algorithm identifies image regions (i.e. bounding boxes) of interest (RoIs) which are then rescaled to fixed di- mensions (normalizing scale and aspect ratio) and fed into a CNN based classifier. The CNN assigns a probability that the region bounds an object of interest or the null class and, via linear regression, identifies an improved bounding box. This approach has demonstrated state-of-the-art re- sults, however, it is very expensive to train and evaluate, re- quiring multiple full CNN evaluations (one per region pro- posal) and an often expensive pre-processing step. Since the majority of CNN computation occurs in the first few layers, Fast R-CNN [3] addressed these issues by applying a shal- low CNN to the image and then, for each region, extracting fixed sized features from the generated feature map for the final classification. In Faster R-CNN [15] the region pro- posal algorithm was integrated into the CNN providing an end-to-end solution, improved timings and demonstrating that both tasks (region proposal and classification) shared similar underlying features. Despite these improvements, to our knowledge, region based CNN\u2019s have not been demon- strated operating near real-time frequencies. In You Only Look Once (YOLO) [14] they depart from the algorithmically defined region based approaches de- scribed above, opting instead for a predefined, regular grid of detectors. In effect they merged the region classification problem into the region proposal network (RPN) first pro- posed in Faster R-CNN. With this approach the CNN is only evaluated once to produce the outcomes for all detectors resulting in significantly reduced training and evaluation times. In Single Shot Detector (SSD) [12] this approach was further refined with an improved network design and training methodology to demonstrate comparable results to the region based methods. We note that the considerable improvements achieved with SSD required scene dependent engineering to manually predefine the most likely set of re- gions within the image to contain an object, a flaw shared with the Faster R-CNN region proposal network. In particu- lar, SSD demonstrated an improvement of 2.7% MAP [12] by the addition of four aspect ratios to the predefined re- gions on the Pascal VOC2007 [1] dataset, highlighting the importance of manual engineering in modern state-of-the- art detector designs. Without going into too much detail, we note that in practice manually engineered solutions typi- cally limit scalability and adaptiveness to different problem sets (without an expensive re-engineering process). The primary differentiator between these methods lies in how each method identifies and treats the regions to be classified. R-CNN based methods sample regions sparsely based on an algorithmic preprocessing step and normalize the region of interest while YOLO based approaches per- form dense sampling with a manually defined grid of de- tectors without image normalization. Often dense methods are well suited to current implementations and, therefore offer a significant timing advantage over sparse methods. However, in this work, we demonstrate a novel model de- sign which combines the ease of training, scene adaptability and classification accuracy of the sparse region-based ap- proaches with the fast training and evaluation of the dense non-region based methods. 1.2 . Probabilistic Object Detection We formulate the probabilistic multiclass detection prob- lem as first estimating the distribution Pr(s|B, I ) where s \u2208 C \u2229 {null} is a random variable indicating the pres- ence of an instance of class c \u2208 C or the null class (in- dicating no instances) which is sufficiently bounded by the box B = {x, y, w, h} and I is the input image (omitted in subsequent derivations). This formulation incorporates the assumption that only a single instance of a class can occupy each bounding box. We note that this definition does not seek to perform instance assignment, but can be used as an input to an algorithm that does e.g . Non-Max Suppression. Given a suitable neural network design we assert that Pr(s|B) can be estimated from training data with class bounding box annotations. However, since the number of unique bounding boxes is given by |B| \u221d X Y W H where (X, Y ) are the number of image positions and (W, H ) the range of bounding box dimensions the naive solu- tion quickly becomes intractable. For instance, assuming the most common settings for the ImageNet dataset, 1000 classes and 224 \u00d7 224 images, and considering all valid bounding boxes within the image, expressing this distribu- tion requires approximately 629 \u00d7 109 values or 2.5TB in 32bit float format. Clearly this is an intractable problem with current hardware. At the cost of localization accuracy, subsampling the output bounding boxes is a valid approach. For instance, by careful dataset dependent manual engineering, Faster R- CNN and YOLO based approaches subsample the distribu- tion to the order of 104 to 105 bounding boxs [14] [15]. These boxes are then refined by estimating only the most likely bounding box in a local region via linear regression. As an alternative to large scale subsampling, we sought to exploit the fact that, due to occlusion and other factors, we expect a very small subset of bounding boxes to contain class instances other than the null class. Subsequently, we have developed a solution based on the state-of-the-art re- gression capabilities of a single end-to-end CNN which es- timates the highly sparse distribution Pr(s|B) in a real-time (or computationally constrained) operational environment. 2. Directed Sparse Sampling (DSS) We use the term Directed Sparse Sampling to refer to the method of a applying a jointly optimized two stage CNN where one stage estimates the likely locations where user- defined interesting values occur and the other sparsely clas- sifies the identified values e.g . in R-CNN based models (including R-FCN and DeNet) we estimate the bounding boxes which are most likely to include a non-null class as- signment, then run a classifer over these bounding boxs. 2.1 . Corner-based RoI Detector Here we introduce the concept of bounding box corner estimation for efficient region-of-interest (RoI) estimation. In our methodology, th
{"name": "1707.02968v2", "contents": [{"tool_name": "djvu", "text": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era Chen Sun1, Abhinav Shrivastava1,2 , Saurabh Singh1 , and Abhinav Gupta1,2 1 Google Research 2 Carnegie Mellon University Abstract The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased compu- tational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we in- crease the dataset size by 10\u00d7 or 100\u00d7? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between \u2018enormous data\u2019 and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we inves- tigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre- training) still holds a lot of promise. One can improve per- formance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the- art results for different vision tasks including image clas- sification, object detection, semantic segmentation and hu- man pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets. 1. Introduction There is unanimous agreement that the current ConvNet revolution is a product of big labeled datasets (specifically, 1M labeled images from ImageNet [35]) and large compu- tational power (thanks to GPUs). Every year we get further increase in computational power (a newer and faster GPU) but our datasets have not been so fortunate. ImageNet, a dataset of 1M labeled images based on 1000 categories, was used to train AlexNet [25] more than five years ago. Curi- 150 300 AlexNet VGG ResNet-50 ResNet-101 Inception ResNet-v2 # Paramaters 6000 12000 2012 2013 2014 2015 2016 G F l o p s # o f L a y e r s # o f I m a g e s ( M ) 1 1.5 Dataset Size Model Size GPU Power Figure 1. The Curious Case of Vision Datasets: While GPU com- putation power and model sizes have continued to increase over the last five years, size of the largest training dataset has surpris- ingly remained constant. Why is that? What would have happened if we have used our resources to increase dataset size as well? This paper provides a sneak-peek into what could be if the dataset sizes are increased dramatically. ously, while both GPUs and model capacity have contin- ued to grow, datasets to train these models have remained stagnant. Even a 101-layer ResNet with significantly more capacity and depth is still trained with 1M images from Im- ageNet circa 2011. Why is that? Have we once again be- littled the importance of data in front of deeper models and computational power? What will happen if we scale up the amount of training data 10\u00d7 or 100\u00d7, will the performance double? This paper takes the first steps towards clearing the clouds of mystery surrounding the relationship between \u2018enormous data\u2019 and deep learning. We exploit the al- 1 a r X i v : 1 7 0 7 . 0 2 9 6 8 v 2 [ c s . C V ] 4 A u g 2 0 1 7 ready existing JFT-image dataset, first introduced by Hin- ton et al. [17] and expanded by [7]. The JFT dataset has more than 300M images that are labeled with 18291 cate- gories. The annotations have been automatically obtained and, therefore, are noisy and not exhaustive. These an- notations have been cleaned using complex algorithms to increase the precision of labels; however there is still ap- proximately 20% error in precision. We will use this data to investigate the nature of relationship between amount of data and performance on vision tasks. Specifically, we will look into the power of data for visual representation learn- ing (pre-training). We evaluate our learned representation on a variety of vision tasks: image classification, object de- tection, semantic segmentation and human pose estimation. Our experiments yield some surprising (and some expected) findings: \u2022 Better Representation Learning Helps! Our first ob- servation is that large-scale data helps in representation learning as evidenced by improvement in performance on each and every vision task we study. This suggests that collection of a larger-scale dataset to study visual pretraining may greatly benefit the field. Our findings also suggest a bright future for unsuper- vised or self-supervised [10, 43] representation learn- ing approaches. It seems the scale of data can over- power noise in the label space. \u2022 Performance increases logarithmically based on volume of training data. We find there is a logarith- mic relationship between performance on vision tasks and the amount of training data used for representa- tion learning. Note that previous papers on large-scale learning [23] have shown diminishing returns even on log-scale. \u2022 Capacity is Crucial: We also observe that to fully ex- ploit 300M images, one needs higher capacity models. For example, in case of ResNet-50 the gain on COCO object detection is much smaller (1.87%) compared to (3%) when using ResNet-152. \u2022 Training with Long-tail: Our data has quite a long tail and yet the representation learning seems to work. This long-tail does not seem to adversely affect the stochastic training of ConvNets (training still con- verges). \u2022 New state of the art results: Finally, our paper presents new state-of-the-art results on several bench- marks using the models learned from JFT-300M. For example, a single model (without any bells and whis- tles) can now achieve 37.4 AP as compared to 34.3 AP on the COCO detection benchmark. 2. Related Work Ever since the seminal work by Krizhevsky et al. [25] showcased the power of Convolutional Neural Networks (ConvNets) on large-scale image recognition task, a lot of work has been done to make them more accurate. A com- mon approach is to increase the complexity of these net- works by increasing the width or depth of these networks. For example, Simonyan and Zisserman [37] proposed the VGG-19 model which uses smaller convolutional filters and has depth of 19 layers. Since then the representational power and depth of these models have continued to grow every year. GoogleNet [39] was a 22-layer network. In this paper, we perform all our experiments with the ResNet models proposed by He et al. [16]. The core idea is to add residual connections between layers which helps in opti- mization of very-deep models. This results in new state- of-the-art performances on a number of recognition tasks. Convolutional neural networks learn a hierarchy of vi- sual representations. These visual representations have been shown to be effective on a wide range of computer vision tasks [1, 4, 14, 22, 29, 33, 36]. Learning these visual representations require large-scale training data. However, the biggest detection and segmentation datasets are still on the order of hundreds of thousands of images. Therefore, most of these approaches employ pre-training. The origi- nal model is learning using million labeled images in Ima- geNet and then further trained on target tasks (fine-tuning) to yield better performance [4, 14, 33]. Huang et al. [18] thoroughly evaluated the influence of multiple ConvNet ar- chitectures on object detection performance, and found that it is closely correlated with the models\u2019 capacity and classi- fication performances on ImageNet. While there has been significant work on increasing the representational capacity of ConvNets, the amount of train- ing data for pre-training has remain kind of fixed over years. The prime reason behind this is the lack of human verified image datasets larger than ImageNet. In order to overcome the bottleneck, there have been recent efforts on visual rep- resentation learning using web-supervision [2, 5, 6, 9, 21, 23, 24, 27] or unsupervised [10, 11, 31, 32, 34, 42, 43] paradigms. However, most of these efforts are still are still exploratory in nature and far lower in performance com- pared to fully-supervised learning. In this paper, we aim to shift the discussion from mod- els to data. Our paper is inspired from several papers which have time and again paid closer look to impact and proper- ties of data rather than models. In 2009, Pereira et al. [30] presented a survey paper to look into impact of data in fields such as natural language processing and computer vision. They argued unlike physics, areas in AI are more likely to see an impact using more data-driven approaches. An- other related work is the empirical study by Torralba and Efros [41] that highlighted the dataset biases in current com- puter vision approaches and how it impacts future research. Specifically, we focus on understanding the relationship between data and visual deep learning. There have been some efforts to understand this relationship. For example, Oquab et al. [28] showed that expanding the training data to cover 1512 labels from ImageNet-14M further improves the object detection performance. Similarly, Huh et al. [19] showed that using a smaller subset of images for training from ImageNet hurts performance. Both these studies also show that selection of categories for training is important and random addition of categories tends to hurt the perfor- mance. But what happens when the number of categories are increased 10x? Do we still need manual selection of categories? Similarly, neither of these efforts demonstrated data effects at significantly larger scale. Some recent work [23, 44] have looked at training Con- vNets with significantly larger data. While [44] looked at geo-localization, [23] utilized the YFCC-100M dataset [40] for representation learning. However, unlike ours, [23] showed plateauing of detection performance when trained on 100M images. Why is that? We believe there could be two possi
{"name": "1711.06396v1", "contents": [{"tool_name": "djvu", "text": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection Yin Zhou Apple Inc yzhou3@apple.com Oncel Tuzel Apple Inc otuzel@apple.com Abstract Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual re- ality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for exam- ple, a bird\u2019s eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifi- cally, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly in- troduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric rep- resentation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection bench- mark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Fur- thermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR. 1. Introduction Point cloud based 3D object detection is an important component of a variety of real-world applications, such as autonomous navigation [11, 14], housekeeping robots [26], and augmented/virtual reality [27]. Compared to image- based detection, LiDAR provides reliable depth informa- tion that can be used to accurately localize objects and characterize their shapes [21, 5]. However, unlike im- ages, LiDAR point clouds are sparse and have highly vari- able point density, due to factors such as non-uniform sampling of the 3D space, effective range of the sensors, occlusion, and the relative pose. To handle these chal- lenges, many approaches manually crafted feature represen- Figure 1. VoxelNet directly operates on the raw point cloud (no need for feature engineering) and produces the 3D detection re- sults using a single end-to-end trainable network. tations for point clouds that are tuned for 3D object detec- tion. Several methods project point clouds into a perspec- tive view and apply image-based feature extraction tech- niques [28, 15, 22]. Other approaches rasterize point clouds into a 3D voxel grid and encode each voxel with hand- crafted features [41, 9, 37, 38, 21, 5]. However, these man- ual design choices introduce an information bottleneck that prevents these approaches from effectively exploiting 3D shape information and the required invariances for the de- tection task. A major breakthrough in recognition [20] and detection [13] tasks on images was due to moving from hand-crafted features to machine-learned features. Recently, Qi et al.[29] proposed PointNet, an end-to- end deep neural network that learns point-wise features di- rectly from point clouds. This approach demonstrated im- pressive results on 3D object recognition, 3D object part segmentation, and point-wise semantic segmentation tasks. In [30], an improved version of PointNet was introduced which enabled the network to learn local structures at dif- ferent scales. To achieve satisfactory results, these two ap- proaches trained feature transformer networks on all input points (\u223c1k points). Since typical point clouds obtained using LiDARs contain \u223c100k points, training the architec- 1 a r X i v : 1 7 1 1 . 0 6 3 9 6 v 1 [ c s . C V ] 1 7 N o v 2 0 1 7 Figure 2. VoxelNet architecture. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. tures as in [29, 30] results in high computational and mem- ory requirements. Scaling up 3D feature learning networks to orders of magnitude more points and to 3D detection tasks are the main challenges that we address in this paper. Region proposal network (RPN) [32] is a highly opti- mized algorithm for efficient object detection [17, 5, 31, 24]. However, this approach requires data to be dense and organized in a tensor structure (e.g . image, video) which is not the case for typical LiDAR point clouds. In this pa- per, we close the gap between point set feature learning and RPN for 3D detection task. We present VoxelNet, a generic 3D detection framework that simultaneously learns a discriminative feature represen- tation from point clouds and predicts accurate 3D bounding boxes, in an end-to-end fashion, as shown in Figure 2. We design a novel voxel feature encoding (VFE) layer, which enables inter-point interaction within a voxel, by combin- ing point-wise features with a locally aggregated feature. Stacking multiple VFE layers allows learning complex fea- tures for characterizing local 3D shape information. Specif- ically, VoxelNet divides the point cloud into equally spaced 3D voxels, encodes each voxel via stacked VFE layers, and then 3D convolution further aggregates local voxel features, transforming the point cloud into a high-dimensional volu- metric representation. Finally, a RPN consumes the vol- umetric representation and yields the detection result. This efficient algorithm benefits both from the sparse point struc- ture and efficient parallel processing on the voxel grid. We evaluate VoxelNet on the bird\u2019s eye view detection and the full 3D detection tasks, provided by the KITTI benchmark [11]. Experimental results show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. We also demonstrate that Voxel- Net achieves highly encouraging results in detecting pedes- trians and cyclists from LiDAR point cloud. 1.1 . Related Work Rapid development of 3D sensor technology has moti- vated researchers to develop efficient representations to de- tect and localize objects in point clouds. Some of the earlier methods for feature representation are [39, 8, 7, 19, 40, 33, 6, 25, 1, 34, 2]. These hand-crafted features yield satisfac- tory results when rich and detailed 3D shape information is available. However their inability to adapt to more complex shapes and scenes, and learn required invariances from data resulted in limited success for uncontrolled scenarios such as autonomous navigation. Given that images provide detailed texture information, many algorithms infered the 3D bounding boxes from 2D images [4, 3, 42, 43, 44, 36]. However, the accuracy of image-based 3D detection approaches are bounded by the accuracy of the depth estimation. Several LIDAR based 3D object detection techniques utilize a voxel grid representation. [41, 9] encode each nonempty voxel with 6 statistical quantities that are de- rived from all the points contained within the voxel. [37] fuses multiple local statistics to represent each voxel. [38] computes the truncated signed distance on the voxel grid. [21] uses binary encoding for the 3D voxel grid. [5] in- troduces a multi-view representation for a LiDAR point cloud by computing a multi-channel feature map in the bird\u2019s eye view and the cylindral coordinates in the frontal view. Several other studies project point clouds onto a per- spective view and then use image-based feature encoding schemes [28, 15, 22]. There are also several multi-modal fusion methods that combine images and LiDAR to improve detection accu- racy [10, 16, 5]. These methods provide improved perfor- mance compared to LiDAR-only 3D detection, particularly for small objects (pedestrians, cyclists) or when the objects are far, since cameras provide an order of magnitude more measurements than LiDAR. However the need for an addi- tional camera that is time synchronized and calibrated with the LiDAR restricts their use and makes the solution more sensitive to sensor failure modes. In this work we focus on LiDAR-only detection. 1.2. Contributions \u2022 We propose a novel end-to-end trainable deep archi- tecture for point-cloud-based 3D detection, VoxelNet, that directly operates on sparse 3D points and avoids information bottlenecks introduced by manual feature engineering. \u2022 We present an efficient method to implement VoxelNet which benefits both from the sparse point structure and efficient parallel processing on the voxel grid. \u2022 We conduct experiments on KITTI benchmark and show that VoxelNet produces state-of-the-art results in LiDAR-based car, pedestrian, and cyclist detection benchmarks. 2. VoxelNet In this section we explain the architecture of VoxelNet, the loss function used for training, and an efficient algo- rithm to implement the network. 2.1. VoxelNet Architecture The proposed VoxelNet consists of three functional blocks: (1) Feature learning network, (2) Convolutional middle layers, and (3) Region proposal network [32], as il- lustrated in Figure 2. We provide a detailed introduction of VoxelNet in the following sections. 2.1 .1 Feature Learning Network Voxel Partition Given a point cloud, we subdivide the 3D space into equally spaced voxels as shown in Figure 2. Sup- pose the point cloud encompasses 3D space with range D, H , W along the Z, Y, X axes respectively. We define each voxel of size vD , vH , and vW accordingly. The resulting 3D voxel grid is of size D0 = D/vD, H 0 = H/vH,W 0 = W/vW . Here, for simplicity, we assume D, H , W are a multiple of vD, vH, vW . Grouping We group the points according to the voxel they reside in. Due to factors such as distance, occlusion, ob- ject\u2019s relative pose, and non-uniform sampling, the LiDAR F u l l y C o n n e c t e d N e u r a l N e t Point-wise Input Point-wise Feature E l e m e n t - w i s e M a x p o o l P o i n t - w i s e C o n c a t e n a t e Locally Aggregated Feature Point-wise concatenated Feat
{"name": "1712.02294v4", "contents": [{"tool_name": "djvu", "text": "Joint 3D Proposal Generation and Object Detection from View Aggregation Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L. Waslander Abstract\u2014 We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod I. INTRODUCTION The remarkable progress made by deep neural networks on the task of 2D object detection in recent years has not transferred well to the detection of objects in 3D. The gap between the two remains large on standard benchmarks such as the KITTI Object Detection Benchmark [1] where 2D car detectors have achieved over 90% Average Precision (AP), whereas the top scoring 3D car detector on the same scenes only achieves 70% AP. The reason for such a gap stems from the difficulty induced by adding a third dimension to the estimation problem, the low resolution of 3D input data, and the deterioration of its quality as a function of distance. Furthermore, unlike 2D object detection, the 3D object detection task requires estimating oriented bounding boxes (Fig. 1). Similar to 2D object detectors, most state-of-the-art deep models for 3D object detection rely on a 3D region proposal generation step for 3D search space reduction. Using region proposals allows the generation of high quality detections via more complex and computationally expensive processing at later detection stages. However, any missed instances at the proposal generation stage cannot be recovered during the following stages. Therefore, achieving a high recall during the region proposal generation stage is crucial for good performance. 1Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L. Waslander are with the Department of Mechanical and Mechatronics Engineering, Faculty of Engineering, University of Waterloo, 200 University Avenue, Waterloo, ON, Canada. Email: (jason.ku@uwaterloo.ca, mfmozifi@uwaterloo.ca, j343lee@uwaterloo.ca, www.aharakeh.com, stevenw@uwaterloo.ca) \u03b8yaw \u03b8yaw X Y X Y Fig. 1: A visual representation of the 3D detection problem from Bird\u2019s Eye View (BEV). The bounding box in Green is used to determine the IoU overlap in the computation of the average precision. The importance of explicit orientation estimation can be seen as an object\u2019s bounding box does not change when the orientation (purple) is shifted by \u00b1\u03c0 radians. Region proposal networks (RPNs) were proposed in Faster-RCNN [2], and have become the prevailing proposal generators in 2D object detectors. RPNs can be considered a weak amodal detector, providing proposals with high recall and low precision. These deep architectures are attractive as they are able to share computationally expensive con- volutional feature extractors with other detection stages. However, extending these RPNs to 3D is a non-trivial task. The Faster R-CNN RPN architecture is tailored for dense, high resolution image input, where objects usually occupy more than a couple of pixels in the feature map. When considering sparse and low resolution input such as the Front View [3] or Bird\u2019s Eye View (BEV) [4] point cloud projections, this method is not guaranteed to have enough information to generate region proposals, especially for small object classes. In this paper, we aim to resolve these difficulties by proposing AVOD, an Aggregate View Object Detection architecture for autonomous driving (Fig. 2). The proposed architecture delivers the following contributions: \u2022 Inspired by feature pyramid networks (FPNs) [5] for 2D object detection, we propose a novel feature extractor that produces high resolution feature maps from LIDAR point clouds and RGB images, allowing for the local- ization of small classes in the scene. \u2022 We propose a feature fusion Region Proposal Network (RPN) that utilizes multiple modalities to produce high- recall region proposals for small classes. \u2022 We propose a novel 3D bounding box encoding that a r X i v : 1 7 1 2 . 0 2 2 9 4 v 4 [ c s . C V ] 1 2 J u l 2 0 1 8 Fig. 2: The proposed method\u2019s architectural diagram. The feature extractors are shown in blue, the region proposal network in pink, and the second stage detection network in green. conforms to box geometric constraints, allowing for higher 3D localization accuracy. \u2022 The proposed neural network architecture exploits 1 \u00d7 1 convolutions at the RPN stage, along with a fixed look-up table of 3D anchor projections, allowing high computational speed and a low memory footprint while maintaining detection performance. The above contributions result in an architecture that delivers state-of-the-art detection performance at a low com- putational cost and memory footprint. Finally, we integrate the network into our autonomous driving stack, and show generalization to new scenes and detection under more extreme weather and lighting conditions, making it a suitable candidate for deployment on autonomous vehicles. II. RELATED WORK Hand Crafted Features For Proposal Generation: Before the emergence of 3D Region Proposal Networks (RPNs) [2], 3D proposal generation algorithms typically used hand- crafted features to generate a small set of candidate boxes that retrieve most of the objects in 3D space. 3DOP [6] and Mono3D [7] uses a variety of hand-crafted geometric features from stereo point clouds and monocular images to score 3D sliding windows in an energy minimization framework. The top K scoring windows are selected as region proposals, which are then consumed by a modified Fast-RCNN [?] to generate the final 3D detections. We use a region proposal network that learns features from both BEV and image spaces to generate higher quality proposals in an efficient manner. Proposal Free Single Shot Detectors: Single shot object detectors have also been proposed as RPN free architectures for the 3D object detection task. VeloFCN [3] projects a LIDAR point cloud to the front view, which is used as an input to a fully convolutional neural network to directly generate dense 3D bounding boxes. 3D-FCN [8] extends this concept by applying 3D convolutions on 3D voxel grids constructed from LIDAR point clouds to generate better 3D bounding boxes. Our two-stage architecture uses an RPN to retrieve most object instances in the road scene, providing better results when compared to both of these single shot methods. VoxelNet [9] extends 3D-FCN further by encoding voxels with point-wise features instead of occupancy values. However, even with sparse 3D convolution operations, VoxelNet\u2019s computational speed is still 3\u00d7 slower than our proposed architecture, which provides better results on the car and pedestrian classes. Monocular-Based Proposal Generation: Another direction in the state-of-the-art is using mature 2D object detectors for proposal generation in 2D, which are then extruded to 3D through amodal extent regression. This trend started with [10] for indoor object detection, which inspired Frustum- based PointNets (F-PointNet) [11] to use point-wise features of PointNet [12] instead of point histograms for extent regression. While these methods work well for indoor scenes and brightly lit outdoor scenes, they are expected to perform poorly in more extreme outdoor scenarios. Any missed 2D detections will lead to missed 3D detections and therefore, the generalization capability of such methods under such extreme conditions has yet to be demonstrated. LIDAR data is much less variable than image data and we show in Section IV that AVOD is robust to noisy LIDAR data and lighting changes, as it was tested in snowy scenes and in low light conditions. Monocular-Based 3D Object Detectors: Another way to utilize mature 2D object detectors is to use prior knowledge to perform 3D object detection from monocular images only. Deep MANTA [13] proposes a many-task vehicle analysis approach from monocular images that optimizes region proposal, detection, 2D box regression, part localization, part visibility, and 3D template prediction simultaneously. The architecture requires a database of 3D models corresponding to several types of vehicles, making the proposed approach hard to generalize to classes where such models do not exist. Deep3DBox [14] proposes to extend 2D object detectors to 3D by exploiting the fact that the perspective projection of a 3D bounding box should fit tightly within its 2D detection window. However, in Section IV, these methods are shown to perform poorly on the 3D detection task compared to methods that use point cloud data. 3D Region Proposal Networks: 3D RPNs have previously been proposed in [15] for 3D object detection from RGBD images. However, up to our knowledge, MV3D [4] is the only architecture that proposed a 3D RPN targeted at au- tonomous driving scenarios. MV3D extends the image based RPN of Faster R-CNN [2] to 3D by corresponding every pixel in the BEV feature map to multiple prior 3D anchors. These anchors are then fed to the RPN to generate 3D proposals that are used to create view-specific feature crops from the BEV, front view of [3], and image view feature maps. A deep fusion scheme is used to combine information from these feature crops to produce the final detection output. However, this RPN architecture does not work well for small object instances in BEV. When downsampled by convolutional feature extractors, small instances will occupy
{"name": "1802.05365v2", "contents": [{"tool_name": "djvu", "text": "Deep contextualized word representations Matthew E. Peters\u2020 , Mark Neumann\u2020 , Mohit Iyyer\u2020, Matt Gardner\u2020 , {matthewp,markn,mohiti,mattg}@allenai.org Christopher Clark\u2217 , Kenton Lee\u2217 , Luke Zettlemoyer\u2020\u2217 {csquared,kentonl,lsz}@cs.washington.edu \u2020Allen Institute for Artificial Intelligence \u2217 Paul G. Allen School of Computer Science & Engineering, University of Washington Abstract We introduce a new type of deep contextual- ized word representation that models both (1) complex characteristics of word use (e.g ., syn- tax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned func- tions of the internal states of a deep bidirec- tional language model (biLM), which is pre- trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, tex- tual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 1 Introduction Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key compo- nent in many neural language understanding mod- els. However, learning high quality representa- tions can be challenging. They should ideally model both (1) complex characteristics of word use (e.g ., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language un- derstanding problems. Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirec- tional LSTM that is trained with a coupled lan- guage model (LM) objective on a large text cor- pus. For this reason, we call them ELMo (Em- beddings from Language Models) representations. Unlike previous approaches for learning contextu- alized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the in- ternal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Combining the internal states in this manner al- lows for very rich word representations. Using in- trinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used with- out modification to perform well on supervised word sense disambiguation tasks) while lower- level states model aspects of syntax (e.g ., they can be used to do part-of-speech tagging). Simultane- ously exposing all of these signals is highly bene- ficial, allowing the learned models select the types of semi-supervision that are most useful for each end task. Extensive experiments demonstrate that ELMo representations work extremely well in practice. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including tex- tual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized rep- resentations using a neural machine translation en- coder. Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform a r X i v : 1 8 0 2 . 0 5 3 6 5 v 2 [ c s . C L ] 2 2 M a r 2 0 1 8 those derived from just the top layer of an LSTM. Our trained models and code are publicly avail- able, and we expect that ELMo will provide simi- lar gains for many other NLP problems.1 2 Related work Due to their ability to capture syntactic and se- mantic information of words from large scale un- labeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-of- the-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single context- independent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword informa- tion (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g ., Neelakantan et al., 2014). Our ap- proach also benefits from subword units through the use of character convolutions, and we seam- lessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised lan- guage model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. 1 http://allennlp.org/elmo Previous work has also shown that different lay- ers of deep biRNNs encode different types of in- formation. For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (S\u00f8gaard and Goldberg, 2016). In an RNN-based encoder-decoder machine trans- lation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2- layer LSTM encoder are better at predicting POS tags then second layer. Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense. We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision. Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using lan- guage models and sequence autoencoders and then fine tune with task specific supervision. In con- trast, after pretraining the biLM with unlabeled data, we fix the weights and add additional task- specific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model. 3 ELMo: Embeddings from Language Models Unlike most widely used word embeddings (Pen- nington et al., 2014), ELMo word representations are functions of the entire input sentence, as de- scribed in this section. They are computed on top of two-layer biLMs with character convolutions (Sec. 3 .1), as a linear function of the internal net- work states (Sec. 3 .2). This setup allows us to do semi-supervised learning, where the biLM is pre- trained at a large scale (Sec. 3.4) and easily incor- porated into a wide range of existing neural NLP architectures (Sec. 3 .3). 3.1 Bidirectional language models Given a sequence of N tokens, (t1 , t2 , ..., tN ), a forward language model computes the probability of the sequence by modeling the probability of to- ken tk given the history (t1 , ..., tk\u22121 ): p(t1,t2,...,tN)= N Y k=1 p(tk|t1,t2,...,tk\u22121). Recent state-of-the-art neural language models (J \u0301ozefowicz et al., 2016; Melis et al., 2017; Mer- ity et al., 2017) compute a context-independent to- ken representation xLM k (via token embeddings or a CNN over characters) then pass it through L lay- ers of forward LSTMs. At each position k , each LSTM layer outputs a context-dependent repre- sentation \u2212 \u2192 hLM k,j wherej=1,...,L.Thetoplayer LSTM output, \u2212 \u2192 hLM k,L , is used to predict the next token tk+1 with a Softmax layer. A backward LM is similar to a forward LM, ex- cept it runs over the sequence in reverse, predict- ing the previous token given the future context: p(t1,t2,...,tN)= N Y k=1 p(tk|tk+1,tk+2,...,tN). It can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations \u2190\u2212 hLM k,j oftkgiven(tk+1,...,tN). A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions: N X k=1 (logp(tk|t1,...,tk\u22121;\u0398x, \u2212 \u2192 \u0398LSTM,\u0398s) +logp(tk|tk+1,...,tN;\u0398x, \u2190\u2212 \u0398LSTM,\u0398s)). We tie the parameters for both the token represen- tation (\u0398x) and Softmax layer (\u0398s) in the forward and backward direction while maintaining sepa- rate parameters for the LSTMs in each direction. Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using complet
