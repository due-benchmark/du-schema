{"name": "1410.2455v3", "contents": [{"tool_name": "djvu", "text": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments Stephan Gouws SGOUWS@GOOGLE.COM Google Inc., Mountain View, CA, USA Yoshua Bengio Dept. IRO, Universit \u0301e de Montr \u0301eal, QC, Canada & Canadian Institute for Advanced Research Greg Corrado Google Inc., Mountain View, CA, USA Abstract We introduce BilBOWA (Bilingual Bag-of- Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel train- ing data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regular- ize two noise-contrastive language models for ef- ficient cross-lingual feature learning. We show that bilingual embeddings learned using the pro- posed model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data. 1. Introduction Raw text data is freely available in many languages, yet labeled data \u2013 e.g. text marked up with parts-of-speech or named-entities \u2013 is expensive and mostly available for English. Although several techniques exist that can learn to map hand-crafted features from one domain to another (Blitzer et al., 2006; Daum \u0301e III, 2009; Pan & Yang, 2010), it is in general non-trivial to come up with good features which generalize well across tasks, and even harder across different languages. It is therefore very desirable to have unsupervised techniques which can learn useful syntactic and semantic features that are invariant to the tasks or lan- Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy- right 2015 by the author(s). guages that we are interested in. Unsupervised distributed representations of words capture important syntactic and semantic information about languages and these techniques have been succesfully applied to a wide range of tasks (Col- lobert et al., 2011; Turian et al., 2010), across many differ- ent languages (Al-Rfou\u2019 et al., 2013). Traditionally, induc- ing these representations involved training a neural network language model (Bengio et al., 2003) which was slow to train. However, contemporary word embedding models are much faster in comparison, and can scale to train on billions of words per day on a single desktop machine (Mnih & Teh, 2012; Mikolov et al., 2013b; Pennington et al., 2014). In all these models, words are represented by learned, real- valued feature vectors referred to as word embeddings and trained from large amounts of raw text. These models have the property that similar embedding vectors are learned for similar words during training. Additionally, the vectors capture rich linguistic relationships such as male-female relationships or verb tenses, as illustrated in Figure 1 (a) and (b). These two properties improve generalization when the embedding vectors are used as features on word- and sentence-level prediction tasks. Distributed representations can also be induced over dif- ferent language-pairs and can serve as an effective way of learning linguistic regularities which generalize across languages, in that words with similar distributional syn- tactic and semantic properties in both languages are rep- resented using similar vectorial representations (i.e. embed nearby in the embedded space, as shown in Figure 1 (c)). This is especially useful for transferring limited label in- formation from high-resource to low-resource languages, and has been demonstrated to be effective for document classification (Klementiev et al., 2012), outperforming a strong machine-translation baseline; as well as named- entity recognition and machine translation (Zou et al., 2013; Mikolov et al., 2013a). a r X i v : 1 4 1 0 . 2 4 5 5 v 3 [ s t a t . M L ] 4 F e b 2 0 1 6 BilBOWA: Fast Bilingual Distributed Representations without Word Alignments Figure 1. (a & b) Monolingual embeddings have been shown to capture syntactic and semantic features such as noun gender (blue) and verb tense (red). (c) The (idealized) goal of crosslingual embeddings is to capture these relationships across two or more languages. Since these techniques are fundamentally data-driven tech- niques, the quality of the learned representations improves as the size of the training data improves (Mikolov et al., 2013b; Pennington et al., 2014). However, as we will discuss in more detail in \u00a72, there are two significant drawbacks associated with current bilingual embedding methods: they are either very slow to train or they can only exploit parallel training data. The former limits the large-scale application of these techniques, while the latter severely limits the amount of available training data, and furthermore introduces a big domain bias into the learning process, since parallel data is typically only easily available for certain narrow domains (such as parliamentary discus- sions). This paper introduces BilBOWA (Bilingual Bag-of-Words without Word Alignments), a simple, scalable technique for inducing bilingual word embeddings with a trivial exten- sion to multilingual embeddings. The model is able to leverage essentially unlimited amounts of monolingual raw text. It furthermore does not require any word-level align- ments, but instead extracts a bilingual signal directly from a limited sample of sentence-aligned, raw-text parallel data (e.g . Europarl) which it uses to align embeddings as they are learned over monolingual training data. Our contribu- tions are the following: \u2022 We introduce a novel, computationally-efficient sam- pled cross-lingual objective (\u201cBilBOWA-loss\u201d) which is employed to align monolingual embeddings as they are being trained in an online setting. The mono- lingual models can scale to large-scale training sets, thereby avoiding training bias, and the BilBOWA- loss only considers sampled bag-of-words sentence- aligned data at each training step, which scales ex- tremely well and also avoids the need for estimating word-alignments (\u00a73.2); \u2022 we experimentally evaluate the induced cross-lingual embeddings on a document-classification (\u00a75.1) and lexical translation task (\u00a75.2), where the method out- performs current state-of-the-art methods, with train- ing time reduced to minutes or hours compared to sev- eral days for prior approaches; \u2022 finally, we make available our efficient C- implementation1 to hopefully stimulate further research on cross-lingual distributed feature learning. 2. Learning Cross-lingual Word Embeddings Monolingual word embedding algorithms (Mikolov et al., 2013b; Pennington et al., 2014) learn useful features about words from raw text (e.g. Fig 1 (a) & (b)). These algo- rithms are trained over large datasets to be able to predict words from the contexts in which they appear. Their work- ing can intuitively be understood as mapping each word to a learned vector in an embedded space, and updating these vectors in an attempt to simultaneously minimize the distance from a word\u2019s vector to the vectors of the words with which it frequently co-occurs. The result of this opti- mization process yields a rich geometrical encoding of the distributional properties of natural language, where words with similar distributional properties cluster together. Due to their general nature, these features work well for several NLP prediction tasks (Collobert et al., 2011; Turian et al., 2010). In the cross-lingual setup, the goal is to learn features which generalize well across different tasks and different languages. The goal is therefore to learn features (embed- dings) for each word such that similar words in each lan- 1 https://github.com/gouwsmeister/bilbowa BilBOWA: Fast Bilingual Distributed Representations without Word Alignments guage are assigned similar embeddings (the monolingual objectives), but additionally we also want similar words across languages to have similar representations (the cross- lingual objective). The latter property allows one to use the learned embeddings as features for training a discrimi- native classifier to predict labels in one language (e.g . top- ics, parts-of-speech, or named-entities) where we have la- belled data, and then directly transfer it to a language for which we do not have much labelled data. From an opti- mization perspective, there are several approaches to how one can optimize these two objectives (our classification): OFFLINE ALIGNMENT: The simplest approach is to opti- mize each monolingual objective separately (i.e. train em- beddings on each language separately using any of the sev- eral available off-the-shelve toolkits), and then enforce the cross-lingual constraints as a separate, disjoint, \u2018alignment\u2019 step. The alignment step consists of learning a transforma- tion for projecting the embeddings of words onto the em- beddings of their translation pairs, obtained from a dictio- nary. This was shown to be a viable approach by (Mikolov et al., 2013a) who learned a linear projection from one em- bedding space to the other. It was extended by (Faruqui & Dyer, 2014), who simultanteously projected source and tar- get language embeddings into a joint space using canonical correlation analysis. The advantage of this approach is that it is very fast to learn the embedding alignments. The main drawback of this approach is that it is not clear that a sin- gle transformation (whether linear or nonlinear) can cap- ture the relationships between all words in the source and target languages, and our improved results on the transla- tion task seem to point to the contrary (\u00a75.2). Furthermore, an accurate dictionary is required for the language-pair and the method considers only one translation per word, which ignores the rich multi-sense polysemy of natural languages. PA R A L LEL- O N LY: Alternatively, one may leverage purely sentence-aligned parallel data and train a model to learn similar representations for the aligned sentences. This is the approach followe
{"name": "1411.0589v3", "contents": [{"tool_name": "djvu", "text": "Modular proximal optimization for multidimensional total-variation regularization \u0301 Alvaro Barbero alvaro.barbero@inv.uam.es Instituto de Ingenier\u0301\u0131a del Conocimiento and Universidad Aut \u0301onoma de Madrid Francisco Tom \u0301as y Valiente 11, Madrid, Spain Suvrit Sra\u2217 suvrit@mit.edu Laboratory for Information and Decision Systems Massachusetts Institute of Technology (MIT), Cambridge, MA Abstract We study TV regularization, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for `p -norm TV. The most important among these is `1 -norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library. 1 Introduction Sparsity impacts the entire data analysis pipeline, touching algorithmic, modeling, as well as practical aspects. Most commonly, sparsity is elicited via `1-norm regularization [20, 84]. However, numerous applications rely on more refined \u201cstructured\u201d notions of sparsity, e.g., groupwise-sparsity [6, 57, 63, 97], hierarchical sparsity [4, 61], gradient sparsity [76, 86, 90], or sparsity over structured \u2018atoms\u2019 [24]. Such regularizers typically arise in optimization problems of the form minx\u2208Rn \u03a6(x) := `(x) + r(x), (1.1) where ` : Rn \u2192 R is a smooth loss function (often convex), while r : Rn \u2192 R \u222a {+\u221e} is a lower semicontinuous, convex, and nonsmooth regularizer that induces sparsity. We focus on instances of (1.1) where r is a weighted anisotropic Total-Variation (TV) regularizer:1 , which, foravectorx\u2208Rnandfixedweightsw\u22650isdefinedas r(x) def =Tv 1 p(w; x) def =  Xn\u22121 j=1 wj |xj+1 \u2212 xj |p 1/p p\u22651. (1.2) More generally, if X is an order-m tensor in R Qm j=1 nj with entries Xi1,i2,...,im (1 \u2264 ij \u2264 nj for 1 \u2264 j \u2264 m); we define the weighted m-dimensional anisotropic TV regularizer as Tv m p (W;X) def = m X k=1 X Ik={i1,...,im}\\ik  nk \u22121 X j=1 wIk ,j |X[k] j+1\u2212X [k] j |pk  1/pk , (1.3) \u2217 An initial version of this work was performed during 2013-14, when the author was with the Max Planck Institute for Intelligent Systems, T \u0308ubingen, Germany, and with Carnegie Mellon University, Pittsburgh. 1We use the term \u201canisotropic\u201d to refer to the specific TV penalties considered in this paper. 1 a r X i v : 1 4 1 1 . 0 5 8 9 v 3 [ s t a t . M L ] 3 0 D e c 2 0 1 7 where X[k] j \u2261 Xi1,...,ik\u22121,j,ik+1,...,im , wIk,j\u22650areweights,andp\u2261[pk\u22651]for1\u2264k\u2264m. IfXisamatrix, expression (1.3) reduces to (note, p, q \u2265 1) Tv 2 p,q(W;X)= n1 X i=1  n2 \u22121 X j=1 w1,j |xi,j +1 \u2212 xi,j |p 1/p + n2 X j=1  n1 \u22121 X i=1 w2,i|xi+1,j \u2212 xi,j |q 1/q , (1.4) These definitions look formidable; already 2D-TV (1.4) or even the simplest 1D-TV (1.2) are fairly complex, which further complicates the overall optimization problem (1.1). Fortunately, this complexity can be \u201clocalized\u201d by invoking prox-operators [65], which are now widely used across machine learning [68, 81]. The main idea of using prox-operators while solving (1.1) is as follows. Suppose \u03a6 is a convex lsc function onasetX\u2282Rn . The prox-operator of \u03a6 is defined as the map prox\u03a6 def =y 7 \u2192 argmin x\u2208X 1 2kx\u2212yk2 2+\u03a6(x) for y\u2208Rn . (1.5) A popular method based on prox-operators is the proximal gradient method (also known as \u2018forward backward splitting\u2019), which performs a gradient (forward) step followed by a proximal (backward) step to iterate xk+1 = prox\u03b7kr(xk \u2212\u03b7k\u2207`(xk)), k =0,1,. ... (1.6) Numerous other proximal methods exist\u2014see e.g ., [13, 27, 46, 66, 79]. To implement the proximal-gradient iteration (1.6) efficiently, we require a subroutine that computes the prox-operator proxr . An additional concern is whether the overall algorithm requires an exact computation of proxr , or merely a moderately inexact computation. This concern is justified: rarely does r admit an exact algorithm for computing proxr . Fortunately, proximal methods easily admit inexactness, e.g., [78\u201380], which allows approximate prox-operators (as long as the approximation is sufficiently accurate). We study both exact and inexact prox-operators in this paper, contingent upon the `p-norm used and on the data dimensionality m. 1.1 Contributions In particular, we review, analyze, implement, and experiment with a variety of fast algorithms. The ensuing contributions of this paper are summarized below. \u2022 Geometric analysis that leads to a new, efficient version of the classic Taut String Method [32], whose origins can be traced back to [10] \u2013 this version turns out to perform better than most of the recently developed TV proximity methods. \u2022 A previously unknown connection between (a variation of) this classic algorithm and Condat\u2019s un- weighted TV method [28]. This connection provides a geometric, more intuitive interpretation and helps us define a hybrid taut-string algorithm that combines the strengths of both methods, while also providing a new efficient algorithm for weighted `1 -norm 1D-TV proximity. \u2022 Efficient prox-operators for general `p-norm (p \u2265 1) 1D-TV. In particular, \u2013 For p = 2, we present a specialized Newton method based on the root-finding strategy of [64], \u2013 For the general p \u2265 1 case we describe both \u201cprojection-free\u201d and projection based first-order methods. \u2022 Scalable proximal-splitting algorithms for computing 2D (1.4) and higher-D TV (1.3) prox-operators. We review an array of methods in the literature that use prox-splitting, and through extensive experi- ments show that a splitting strategy based on alternating reflections is the most effective in practice. Furthermore, this modular construction of 2D and higher-D TV solvers allows reuse of our fast 1D-TV routines and exploitation of the massive parallelization inherent in matrix and tensor TV. 2 \u2022 The final most important contribution of our paper is a well-tuned, multi-threaded open-source C++, Matlab and Python implementation of all the reviewed and developed methods.2 To complement our algorithms, we illustrate several applications of TV prox-operators to: (i) image and video denoising; (ii) image deconvolution; and (iii) four variants of fused-lasso. Note: We have invested great efforts to ensure reproducibility of our results. In particular, given the vast attention that TV problems have received in the literature, we believe it is valuable to both users of TV and other researchers to have access to our code, datasets, and scripts, to independently verify our claims, if desired.3 1.2 Related work The literature on TV is too large to permit a comprehensive review here. Instead, we mention the most directly related work to help place our contributions in perspective. We focus on anisotropic-TV (in the sense of [16]), in contrast to isotropic-TV [76]. Isotropic TV regularization arises frequently in image denoising and signal processing, and quite a few TV-based denoising algorithms exist [98, see e.g .] . The anisotropic TV regularizers Tv 1D 1 and Tv2D 1,1 arise in image denoising and deconvolution [31], in the fused-lasso [86], in logistic fused-lasso [50], in change-point detection [39], in graph-cut based image segmentation [21], in submodular optimization [43]; see also the related work in [89]. This broad applicability and importance of anisotropic TV is the key motivation towards developing carefully tuned proximity operators. There is a rich literature of methods tailored to anisotropic TV, e.g., those developed in the context of fused-lasso [34, 60], graph-cuts [21], ADMM-style approaches [27, 91], or fast methods based on dynamic programming [45] or KKT conditions analysis [28]. However, it seems that anisotropic TV norms other than `1 has not been studied much in the literature, although recognized as a form of Sobolev semi-norms [70]. For 1D-TV and for the particular `1 norm, there exist several direct methods that are exceptionally fast. We treat this problem in detail in Section 2, and hence refer the reader to that section for discussion of closely related work on fast solvers. We note here, however, that in contrast to many of the previous fast solvers, our solvers allow weights, a capability that can be very important in applications [43]. Regarding 2D-TV, Goldstein T. [36] presented a so-called \u201cSplit-Bregman\u201d (SB). It turns out that this method is essentially a variant of the well-known ADMM method. In contrast to the 2D approach presented here, the SB strategy followed by Goldstein T. [36] is to rely on `1 -soft thresholding substeps instead of 1D-TV substeps. From an implementation viewpoint, the SB approach is somewhat simpler, but not necessarily more accurate. Incidentally, sometimes such direct ADMM approaches turn out to be less effective than ADMM methods that rely on more complex 1D-TV prox-operators [71]. It is worth highlighting that it is not just proximal solvers such as FISTA [13], SpaRSA [93], SALSA [1], TwIST [16], T R I P [46], that can benefit from our fast prox-operators. All other 2D and higher-D TV solvers, e.g., [95], as well as the recent ADMM based trend-filtering solvers of Tibshirani [87] immediately benefit, not only in speed but also by gaining the ability to solve weighted
{"name": "1506.01911v3", "contents": [{"tool_name": "djvu", "text": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video Lionel Pigou, A\u0308aron van den Oord\u2217 , Sander Dieleman\u2217 , Mieke Van Herreweghe & Joni Dambre {lionel.pigou,aaron.vandenoord,sander.dieleman, mieke.vanherreweghe, joni.dambre}@ugent.be Ghent University February 11, 2016 Abstract Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results. 1 Introduction Gesture recognition is one of the core components in the thriving research field of human- computer interaction. The recognition of distinct hand and arm motions is becoming in- creasingly important, as it enables smart interactions with electronic devices. Furthermore, gesture identification in video can be seen as a first step towards sign language recognition, where even subtle differences in motion can play an important role. Some examples that complicate the identification of gestures are changes in background and lighting due to the varying environment, variations in the performance and speed of the gestures, different clothes worn by the performers and different positioning relative to the camera. Moreover, regular hand motion or out-of-vocabulary gestures should not to be confused with one of the target gestures. Convolutional neural networks (CNNs) (LeCun et al., 1998) are the de facto standard approach in computer vision. CNNs have the ability to learn complex hierarchies with increasing levels of abstraction while being end-to-end trainable. Their success has had a huge impact on vision based applications like image classification (Krizhevsky et al., 2012), object detection \u2217 Now at Google DeepMind. 1 a r X i v : 1 5 0 6 . 0 1 9 1 1 v 3 [ c s . C V ] 1 0 F e b 2 0 1 6 (Sermanet et al., 2013), human pose estimation (Toshev & Szegedy, 2014) and many more. A video can be seen as an ordered collection of images. Classifying a video frame by frame with a CNN is bound to ignore motion characteristics, as there is no integration of temporal information. Depending on the task at hand, aggregating the spatial features produced by the CNN with temporal pooling can be a viable strategy (Karpathy et al., 2014; Ng et al., 2015). As we\u2019ll show in this paper, however, this method is of limited use for gesture recognition. Apart from a collection of frames, a video can also be seen as a time series. Some of the most successful models for time series classification are recurrent neural networks (RNNs) with either standard cells or long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997). Their ability to learn dynamic temporal dependencies has allowed researchers to achieve breakthrough results in e.g. speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014) and image captioning (Vinyals et al., 2015). Before feeding video to recurrent models, we need to incorporate some form of spatial or spatiotemporal feature ex- traction. This motivates the concept of combining CNNs with RNNs. CNNs have unparalleled spatial (and spatiotemporal with added temporal convolutions) feature extraction capabilities, while adding recurrence ensures the modeling of feature evolution over time. For general video classification datasets like UCF-101 (Soomro et al., 2012), Sports-1M (Karpathy et al., 2014) or HMDB-51 (Kuehne et al., 2011), the temporal aspect is of less importance compared to a gesture recognition dataset. For example, the appearance of a violin almost certainly suggests the target class is \u201cplaying violin\u201d, as no other class involves a violin. The model has no need to capture motion information for this particular example. That being said, there are some categories where modeling motion in some way or another is always beneficial. In the case of gesture recognition, however, motion plays a more critical role. Many gestures are not only defined by their spatial hand and/or arm placement, but also by their motion pattern. In this work, we explore a variety of end-to-end trainable deep networks for video classification applied to frame-wise gesture recognition with the Montalbano dataset that was introduced in the ChaLearn LAP 2014 Challenge (Escalera et al., 2014). We study two ways of capturing the temporal structure of these videos. The first method involves temporal convolutions to enable the learning of motion features. The second method introduces recurrence to our networks, which allows the modeling of temp oral dynamics, which plays an essential role in gesture recognition. 2 Related Work An extensive evaluation of CNNs on general video classification is provided by Karpathy et al. (2014) using the Sports-1M dataset. They compare different frame fusion methods to a baseline single-frame architecture and conclude that their best fusion strategy only modestly improves the accuracy of the baseline. Their work is extended by Ng et al. (2015), who show that LSTMs achieve no improvements over a temporal feature pooling scheme on the UCF-101 dataset for human action classification and only marginal improvements on the Sports-1M dataset. For this reason, the single-frame and the temporal pooling architectures are important baseline models. Another way to capture motion is to convert a video stream to a dense optical flow. This is a 2 way to represent motion spatially by estimating displacement vectors of each pixel. It is a core component in the two-stream architecture described by Simonyan & Zisserman (2014) and is used for human pose estimation (Jain et al., 2014), for global video descriptor learning (Ng et al., 2015) and for video captioning (Venugopalan et al., 2015). We have not experimented with optical flow, because (i) it has a greater computational preprocessing complexity and (ii) our models should implicitly learn to infer motion features in an end-to-end fashion, so we chose not to engineer them. Neverova et al. (2014) present an extended overview of their winning solution for the ChaLearn LAP 2014 gesture recognition challenge and achieve a state-of-the-art score on the Montalbano dataset. They propose a multi-modal \u2018ModDrop\u2019 network operating at three temporal scales and use an ensemble method to merge the features at different scales. They also developed a new training strategy, ModDrop, that makes the network\u2019s predictions robust to missing or corrupted channels. Most of the constituent parts in our architectures have been used before in other work for different purposes. Learning motion features with three-dimensional convolution layers has been studied by Ji et al. (2013) and Taylor et al. (2010) to classify short clips of human actions on the KTH dataset. Baccouche et al. (2011) prop osed including a two-step scheme to model the temporal evolution of learned features with an LSTM. Finally, the combination of a CNN with an RNN has been used for speech recognition (Hannun et al., 2014), image captioning (Vinyals et al., 2015) and video narration (Donahue et al., 2015). 3 Architectures In this section, we briefly describe the different architectures we investigate for gesture recognition in video. An overview of the models is depicted in Figure 1. Note that we pay close attention to the comparability of the network structures. The number of units in the fully connected layers and the number of cells in the recurrent models are optimized based on validation results for each network individually. All other hyper-parameters mentioned in this section and in Section 4.2 are optimized for the temporal pooling architecture. As a result, improvements over our baseline models are caused by architectural differences rather than better optimization, other hyper-parameters or preprocessing. 3.1 Baseline Models Single-Frame The single-frame architecture (Figure 1a) worked well for general video classification (Karpathy et al., 2014), but is not a very fitting solution for our frame-wise gesture recognition setting. Nevertheless, this will give us an indication on how much static images contribute to the recognition. It has 3\u00d73 convolution kernels in every layer. Two convolutional layers are stacked before performing max-pooling on non-overlapping 2\u00d72 spatial regions. The shorthand notation of the full architecture is as follows: C (16) - C(16) - P - C(32) - C(32) - P - C(64) - C(64) - P - C(128) - C(128) - P - D(2048) - D(2048) - S , where C (nc) denotes a convolutional layer with nc feature maps, P a max-p ooling layer, D(nd ) a fully connected layer with nd units and S a softmax classifier. We deploy leaky rectified linear units (leaky ReLUs) in every layer. Their activation function is defined as a : x 7 \u2192 max(\u03b1x, x), 3 CNN CNN CNN ... (a) Single-Frame One to One CNN CNN CNN ... Temp Pool (b) Temporal Pooling Many to One CNN CNN CNN ... ... ... (c) RNN Many to Many Conv ... Temp Conv Conv ... Temp Conv MP ... Temp MP . . . . . . \u00d7L (d) Temporal Convolutions Many to One Conv ... Temp Conv Conv ... Temp Conv MP ... ... ... \u00d7L . . . (e) Temporal Convolutions + RNN Many to Many Figure 1: Overview (a) Single-frame CNN architecture. (b) Temporal feature pooling network (max- or mean -pooling), spanning multiple video frames. (c) Model with bidirectional recurrence. (d) Adding temporal convolutions and three-dimensional
{"name": "1507.02159v1", "contents": [{"tool_name": "djvu", "text": "a r X i v : 1 5 0 7 . 0 2 1 5 9 v 1 [ c s . C V ] 8 J u l 2 0 1 5 Towards Good Practices for Very Deep Two-Stream ConvNets Limin Wang1 Yuanjun Xiong1 Zhe Wang2 Yu Qiao2 1 Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong 2 Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China {07wanglimin,bitxiong,buptwangzhe2012}@gmail.com, yu.qiao@siat.ac .cn Abstract Deep convolutional networks have achieved great suc- cess for object recognition in still images. However, for ac- tion recognition in videos, the improvement of deep convo- lutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream ConvNets [12]) are relatively shallow compared with those very deep models in image domain (e.g . VGGNet [13], GoogLeNet [15]), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the ImageNet dataset, and thus it will be easy to over-fit on the training dataset. To address these issues, this report presents very deep two-stream ConvNets for action recognition, by adapting recent very deep architectures into video domain. How- ever, this extension is not easy as the size of action recog- nition is quite small. We design several good practices for the training of very deep two-stream ConvNets, namely (i) pre-training for both spatial and temporal nets, (ii) smaller learning rates, (iii) more data augmentation techniques, (iv) high drop out ratio. Meanwhile, we extend the Caffe tool- box into Multi-GPU implementation with high computa- tional efficiency and low memory consumption. We verify the performance of very deep two-stream ConvNets on the dataset of UCF101 and it achieves the recognition accuracy of 91.4%. 1. Introduction Human action recognition has become an important problem in computer vision and received a lot of research interests in this community [12, 16, 19]. The problem of action recognition is challenging due to the large intra-class variations, low video resolution, high dimension of video data, and so on. The past several years have witnessed great progress on action recognition from short clips [8, 9, 12, 16, 17, 18, 19]. These research works can be roughly categorized into two types. The first type of algorithm focuses on the hand- crafted local features and Bag of Visual Words (BoVWs) representation. The most successful example is to extract improved trajectory features [16] and employ Fisher vector representation [11]. The second type of algorithm utilizes deep convolutional networks (ConvNets) to learn video rep- resentation from raw data (e.g . RGB images or optical flow fields) and train recognition system in an end-to-end man- ner. The most competitive deep model is the two-stream ConvNets [12]. However, unlike image classification [7], deep ConvNets did not yield significant improvement over these traditional methods. We argue that there are two possible reasons to explain this phenomenon. First, the concept of action is more complex than object and it is relevant to other high- level vision concepts, such as interacting object, scene con- text, human pose. Intuitively, the more complicated prob- lem will need the model of higher complexity. However, the current two-stream ConvNets are relatively shallow (5 convolutional layers and 3 fully-connected layers) com- pared with those successful models in image classification [13, 15]. Second, the dataset of action recognition is ex- tremely small compared the ImageNet dataset [1]. For ex- ample, the UCF101 dataset [14] only contains 13, 320 clips. However, these deep ConvNets always require a huge num- ber of training samples to tune the network weights. In order to address these issues, this report presents very deep two-stream ConvNets for action recognition. Very deep two-stream ConvNets contain high modeling capacity and are capable of handling the large complexity of action classes. However, due to the second problem above, train- ing very deep models in such a small dataset is much chal- lenging due to the over-fitting problem. We propose several good practices to make the training of very deep two-stream ConvNets stable and reduce the effect of over-fitting. By carefully training our proposed very deep ConvNets on the action dataset, we are able to achieve the state-of-the-art performance on the dataset of UCF101. Meanwhile, we ex- tend the Caffe toolbox [4] into multi-GPU implementation 1 with high efficiency and low memory consumption. The remainder of this report is organized as follows. In Section 2, we introduce our proposed very deep two-stream ConvNets in details, including network architectures, train- ing details, testing strategy. We report our experimental re- sults on the dataset of UCF101 in Section 3. Finally, we conclude our report in Section 4. 2. Very Deep Two-stream ConvNets In this section, we give a detailed description of our pro- posed method. We first introduce the architectures of very deep two-stream ConvNets. After that, we present the train- ing details, which are very important to reduce the effect of over-fitting. Finally, we describe our testing strategies for action recognition. 2.1. Network architectures Network architectures are of great importance in the de- sign of deep ConvNets. In the past several years, many famous network structures have been proposed for im- age classification, such as AlexNet [7], ClarifaiNet [22], GoogLeNet [15], VGGNet [13], and so on. Some trends emerge during the evolution of from AlexNet to VGGNet: smaller convolutional kernel size, smaller convolutional strides, and deeper network architectures. These trends have turned out to be effective on improving object recogni- tion performance. However, their influence on action recog- nition has not be fully investigated in video domain. Here, we choose two latest successful network structures to de- sign very deep two-stream ConvNets, namely GoogLeNet and VGGNet. GoogLeNet. It is essentially a deep convolutional net- work architecture codenamed Inception, whose basic idea is Hebbian principle and the intuition of multi-scale process- ing. An important component in Inception network is the Inception module. Inception module is composed of multi- ple convolutional filters with different sizes alongside each other. In order to speed up the computational efficiency, 1 \u00d7 1 convolutional operation is chosen for dimension re- duction. GoogLeNet is a 22-layer network consisting of In- ception modules stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of grid. More details can be found in its original paper [15]. VGGNet. It is a new convolutional architecture with smaller convolutional size (3 \u00d7 3), smaller convolutional stride (1 \u00d7 1), smaller pooling window (2 \u00d7 2), deeper structure (up to 19 layers). The VGGNet systematically investigates the influence of network depth on the recog- nition performance, by building and pre-training deeper ar- chitectures based on the shallower ones. Finally, two suc- cessful network structures are proposed for the ImageNet challenge: VGG-16 (13 convolutional layers and 3 fully- connected layers) and VGG-19 (16 convolutional layers and 3 fully-connected layers). More details can be found in its original paper [13]. Very Deep Two-stream ConvNets. Following these successful architectures in object recognition, we adapt them to the design of two-stream ConvNets for action recognition in videos, which we called very deep two- stream ConvNets. We empirically study both GoogLeNet and VGG-16 for the design of very deep two-stream Con- vNets. The spatial net is built on a single frame image (224 \u00d7 224 \u00d7 3) and therefore its architecture is the same as those for object recognition in image domain. The input of temporal net is 10-frame stacking of optical flow fields (224 \u00d7 224 \u00d7 20) and thus the convolutional filters in the first layer are different from those of image classification models. 2.2. Network training Here we describe how to train very deep two-stream ConvNets on the UCF101 dataset. The UCF101 dataset contains 13, 320 video clips and provides 3 splits for eval- uation. For each split, there are around 10, 000 clips for training and 3300 clips for testing. As the training dataset is extremely small and the concept of action is relatively com- plex, training very deep two-stream ConvNets is quite chal- lenging. From our empirical explorations, we discover sev- eral good practices for training very deep two-stream Con- vNets as follows. Pre-training for Two-stream ConvNets. Pre-training has turned out to be an effective way to initialize deep Con- vNets when there is not enough training samples available. For spatial nets, as in [12], we choose the ImageNet mod- els as the initialization for network training. For temporal net, its input modality are optical flow fields, which capture the motion information and are different from static RGB images. Interestingly, we observe that it still works well by pre-training temporal nets with ImageNet model. In order to make this pre-training reasonable, we make several mod- ifications on optical flow fields and ImageNet model. First, we extract optical flow fields for each video and discretize optical flow fields into interval of [0, 255] by a linear trans- formation. Second, as the input channel number for tempo- ral nets is different from that of spatial nets (20 vs. 3), we average the ImageNet model filters of first layer across the channel, and then copy the average results 20 times as the initialization of temporal nets. Smaller Learning Rate. As we pre-trained the two- stream ConvNets with ImageNet model, we use a smaller learning rate compared with original training in [12]. Specifically, we set the learning rate as follows: \u2022 For temporal net, the learning rate starts with 0.005, decreases to its 1/10 every 10,000 iterations, stops at 30,000 iterations. \u2022 For spatial net, the learning rate starts with 0.001, decreases to its 1/10 every 4,000
{"name": "1603.01417v1", "contents": [{"tool_name": "djvu", "text": "Dynamic Memory Networks for Visual and Textual Question Answering Caiming Xiong*, Stephen Merity*, Richard Socher {CMXIONG,SMERITY,RICHARD}METAMIND.IO MetaMind, Palo Alto, CA USA *indicates equal contribution. Abstract Neural network architectures with memory and attention mechanisms exhibit certain reason- ing capabilities required for question answering. One such architecture, the dynamic memory net- work (DMN), obtained high accuracy on a vari- ety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we intro- duce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision. 1. Introduction Neural network based methods have made tremendous progress in image and text classification (Krizhevsky et al., 2012; Socher et al., 2013b). However, only recently has progress been made on more complex tasks that require logical reasoning. This success is based in part on the addition of memory and attention components to complex neural networks. For instance, memory networks (Weston et al., 2015b) are able to reason over several facts written in natural language or (subject, relation, object) triplets. At- tention mechanisms have been successful components in both machine translation (Bahdanau et al., 2015; Luong et al., 2015) and image captioning models (Xu et al., 2015). The dynamic memory network (Kumar et al., 2015) (DMN) is one example of a neural network model that has both a memory component and an attention mechanism. The DMN yields state of the art results on question answer- ing with supporting facts marked during training, sentiment analysis, and part-of-speech tagging. We analyze the DMN components, specifically the input Answer Question Input Module Answer Question Input Module (a) Text Question-Answering (b) Visual Question-Answering What kind of tree is in the background? Kitchen Palm John moved to the garden. John got the apple there. John moved to the kitchen. Sandra got the milk there. John dropped the apple. John moved to the office. Where is the apple? Episodic Memory Attention Mechanism Memory Update Episodic Memory Attention Mechanism Memory Update Figure 1. Question Answering over text and images using a Dy- namic Memory Network. module and memory module, to improve question answer- ing. We propose a new input module which uses a two level encoder with a sentence reader and input fusion layer to allow for information flow between sentences. For the memory, we propose a modification to gated recurrent units (GRU) (Chung et al., 2014). The new GRU formulation in- corporates attention gates that are computed using global knowledge over the facts. Unlike before, the new DMN+ model does not require that supporting facts (i.e . the facts that are relevant for answering a particular question) are labeled during training. The model learns to select the im- portant facts from a larger set. In addition, we introduce a new input module to represent images. This module is compatible with the rest of the DMN architecture and its output is fed into the memory module. We show that the changes in the memory module that improved textual question answering also improve vi- sual question answering. Both tasks are illustrated in Fig. 1 . 2. Dynamic Memory Networks We begin by outlining the DMN for question answering and the modules as presented in Kumar et al. (2015). The DMN is a general architecture for question answering (QA). It is composed of modules that allow different as- pects such as input representations or memory components to be analyzed and improved independently. The modules, depicted in Fig. 1, are as follows: a r X i v : 1 6 0 3 . 0 1 4 1 7 v 1 [ c s . N E ] 4 M a r 2 0 1 6 Dynamic Memory Networks for Visual and Textual Question Answering Input Module: This module processes the input data about which a question is being asked into a set of vectors termed facts, represented asF = [f1, . . . , fN], whereN is the total number of facts. These vectors are ordered, resulting in ad- ditional information that can be used by later components. For text QA in Kumar et al. (2015), the module consists of a GRU over the input words. As the GRU is used in many components of the DMN, it is useful to provide the full definition. For each time step i with input xi and previous hidden state hi\u22121 , we compute the updated hidden state hi = GRU (xi, hi\u22121 ) by ui=\u03c3  W(u) xi + U(u)hi\u22121 + b(u)   (1) ri=\u03c3  W(r) xi + U(r)hi\u22121 + b(r)   (2) \u0303 hi = tanh  Wxi+ri\u25e6Uhi\u22121+b(h)   (3) hi = ui\u25e6\u0303hi+(1\u2212ui)\u25e6hi\u22121 (4) where \u03c3 is the sigmoid activation function, \u25e6 is an element- wise product, W (z) ,W (r) ,W\u2208RnH\u00d7nI,U (z) ,U (r) ,U\u2208 RnH \u00d7nH , nH is the hidden size, and nI is the input size. Question Module: This module computes a vector repre- sentation q of the question, where q \u2208 RnH is the final hidden state of a GRU over the words in the question. Episodic Memory Module: Episode memory aims to re- trieve the information required to answer the question q from the input facts. To improve our understanding of both the question and input, especially if questions require transitive reasoning, the episode memory module may pass over the input multiple times, updating episode memory af- ter each pass. We refer to the episode memory on the tth pass over the inputs as mt , where mt \u2208 RnH , the initial memory vector is set to the question vector: m0 = q . The episodic memory module consists of two separate components: the attention mechanism and the memory up- date mechanism. The attention mechanism is responsible for producing a contextual vector ct, where ct \u2208 RnH is a summary of relevant input for pass t, with relevance inferred by the question q and previous episode memory mt\u22121 . The memory update mechanism is responsible for generating the episode memory mt based upon the contex- tual vector ct and previous episode memory mt\u22121 . By the final pass T , the episodic memory mT should contain all the information required to answer the question q. Answer Module: The answer module receives both q and mT to generate the model\u2019s predicted answer. For simple answers, such as a single word, a linear layer with softmax activation may be used. For tasks requiring a sequence out- put, an RNN maybe usedtodecode a = [q;mT],thecon- catenation of vectors q and mT , to an ordered set of tokens. The cross entropy error on the answers is used for training and backpropagated through the entire network. 3. Improved Dynamic Memory Networks: DMN+ We propose and compare several modeling choices for two crucial components: input representation, attention mecha- nism and memory update. The final DMN+ model obtains the highest accuracy on the bAbI-10k dataset without sup- porting facts and the VQA dataset (Antol et al., 2015). Sev- eral design choices are motivated by intuition and accuracy improvements on that dataset. 3.1. Input Module for Text QA In the DMN specified in Kumar et al. (2015), a single GRU is used to process all the words in the story, extracting sen- tence representations by storing the hidden states produced at the end of sentence markers. The GRU also provides a temporal component by allowing a sentence to know the content of the sentences that came before them. Whilst this input module worked well for bAbI-1k with supporting facts, as reported in Kumar et al. (2015), it did not perform well on bAbI-10k without supporting facts (Sec. 6.1). We speculate that there are two main reasons for this per- formance disparity, all exacerbated by the removal of sup- porting facts. First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU. Input Fusion Layer For the DMN+, we propose replacing this single GRU with two different components. The first component is a sen- tence reader, responsible only for encoding the words into a sentence embedding. The second component is the input fusion layer, allowing for interactions between sentences. This resembles the hierarchical neural auto-encoder archi- tecture of Li et al. (2015) and allows content interaction between sentences. We adopt the bi-directional GRU for this input fusion layer because it allows information from both past and future sentences to be used. As gradients do not need to propagate through the words between sen- tences, the fusion layer also allows for distant supporting sentences to have a more direct interaction. Fig. 2 shows an illustration of an input module, where a positional encoder is used for the sentence reader and a bi-directional GRU is adopted for the input fusion layer. Each sentence encoding fi is the output of an encoding scheme taking the word tokens [wi 1,...,wi Mi ], where Mi is the length of the sentence. Dynamic Memory Networks for Visual and Textual Question Answering Input fusion layer S e n t e n c e r e a d e r Facts GRU f1 f1 w 1 w 2 w 3 w 4 GRU P o s i t i o n a l E n c o d e r GRU f2 f2 w 1 w 2 w 3 w 4 GRU P o s i t i o n a l E n c o d e r GRU f3 f3 w 1 w 2 w 3 w 4 GRU P o s i t i o n a l E n c o d e r 1 1 1 1 2 2 2 2 3 3 3 3 Textual Input Module Figure 2. The input module with a \u201cfusion layer\u201d, where the sen- tence reader encodes the sentence and the bi-directional GRU al- lows information to flow between sentences. The sentence reader could be based on any variety of encoding schemes. We selected positional encoding de- scribed in Sukhbaatar et al. (2015) to allow for a compari- son to their work. GRUs and LSTMs were also considered but required more c
