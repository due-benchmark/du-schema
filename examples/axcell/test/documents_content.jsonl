{"name": "1312.6173v4", "contents": [{"tool_name": "djvu", "text": "Multilingual Distributed Representations without Word Alignment Karl Moritz Hermann and Phil Blunsom Department of Computer Science University of Oxford Oxford, OX1 3QD, UK {karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk Abstract Distributed representations of meaning are a natural way to encode covariance relationships between words and phrases in NLP. By overcoming data sparsity problems, as well as providing information about semantic relatedness which is not available in discrete representations, distributed representations have proven useful in many NLP tasks. Recent work has shown how compositional semantic representations can successfully be applied to a number of monolingual applica- tions such as sentiment analysis. At the same time, there has been some initial success in work on learning shared word-level representations across languages. We combine these two approaches by proposing a method for learning distributed representations in a multilingual setup. Our model learns to assign similar embed- dings to aligned sentences and dissimilar ones to sentence which are not aligned while not requiring word alignments. We show that our representations are seman- tically informative and apply them to a cross-lingual document classification task where we outperform the previous state of the art. Further, by employing parallel corpora of multiple language pairs we find that our model learns representations that capture semantic relationships across languages for which no parallel data was used. 1 Introduction Distributed representations of words are increasingly being used to achieve high levels of generalisa- tion within language modelling tasks. Successful applications of this approach include word-sense disambiguation, word similarity and synonym detection (e.g. [10, 27]). Subsequent work has also attempted to learn distributed semantics of larger structures, allowing us to apply distributed rep- resentation to tasks such as sentiment analysis or paraphrase detection (i.a. [1, 3, 12, 14, 21, 25]). At the same time a second strand of work has focused on transferring linguistic knowledge across languages, and particularly from English into low-resource languages, by means of distributed rep- resentations at the word level [13, 16]. Currently, work on compositional semantic representations focuses on monolingual data while the cross-lingual work focuses on word level representations only. However, it appears logical that these two strands of work should be combined as there exists a plethora of parallel corpora with aligned data at the sentence level or beyond which could be exploited in such work. Further, sentence aligned data provides a plausible concept of semantic similarity, which can be harder to define at the word level. Consider the case of alignment between a German compound noun (e.g. \u201cSchwerlastverkehr\u201d) and its English equivalent (\u201cheavy goods vehicle traffic\u201d). Semantic alignment at the phrase level here appears far more plausible than aligning individual tokens for semantic transfer. 1 a r X i v : 1 3 1 2 . 6 1 7 3 v 4 [ c s . C L ] 2 0 M a r 2 0 1 4 Using this rationale, and building on both work related to learning cross-lingual embeddings as well as to compositional semantic representations, we introduce a model that learns cross-lingual em- beddings at the sentence level. In the following section we will briefly discuss prior work in these two fields before going on to describe the bilingual training signal that we developed for learning multilingual compositional embeddings. Subsequently, we will describe our model in greater detail as well as its training procedure and experimental setup. Finally, we perform a number of evalua- tions and demonstrate that our training signal allows a very simple compositional vector model to outperform the state of the art on a task designed to evaluate its ability to transfer semantic informa- tion across languages. Unlike other work in this area, our model does not require word aligned data. In fact, while we evaluate our model on sentence aligned data in this paper, there is no theoretical requirement for this and technically our algorithm could also be applied to document-level parallel data or even comparable data only. 2 Models of Compositional Distributed Semantics In the case of representing individual words as vectors, the distributional account of semantics pro- vides a plausible explanation of what is encoded in a word vector. This follows the idea that the meaning of a word can be determined by \u201cthe company it keeps\u201d [11], that is by the context it ap- pears in. Such context can easily be encoded in vectors using collocational methods, and is also underlying other methods of learning word embeddings [7, 20]. For a number of important problems, semantic representations of individual words do not suffice, but instead a semantic representation of a larger structure\u2014e.g . a phrase or a sentence\u2014is required. This was highlighted in [10], who proposed a mechanism for modifying a word\u2019s representation based on its individual context. The distributional account of semantics can, due to sparsity, not be applied to such larger linguistic units. A notable exception perhaps is Baroni and Zamparelli [1], who learned distributional representations for adjective noun pairs using a collocational approach on a corpus of unprecedented size. The bigram representations learned from that corpus were subsequently used to learn lexicalised composition functions for the constituent words. Most alternative attempts to extract such higher-level semantic representations have focused on learning composition functions that represent the semantics of a larger structure as a function of the representations of its parts. [21] provides an evaluation of a number of simple composition func- tions applied to bigrams. Applied recursively, such approaches can then easily be reconciled with the co-occurrence based word level representations. There are a number of proposals motivating such recursive or deep composition models. Notably, [3] propose a tensor-based model for semantic composition and, similarly, [4] develop a framework for semantic composition by combining dis- tributional theory with pregroup grammars. The latter framework was empirically evaluated and supported by the results in [12]. More recently, various forms of recursive neural networks have successfully been used for semantic composition and related tasks such as sentiment analysis. Such models include recursive autoencoders [24], matrix-vector recursive neural networks [25], untied recursive neural networks [14] or convolutional networks [15]. 2.1 Multilingual Embeddings Much research has been devoted to the task of inducing distributed semantic representations for single languages. In particular English, with its large number of annotated resources, has enjoyed most attention. Recently, progress has been made at representation learning for languages with fewer available resources. Klementiev et al. [16] described a form of multitask learning on word-aligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. [13], proposed a method for inducing cross-lingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. This approach has recently been extended by [18, 19], who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another. Is was demonstrated that this approach can be applied to improve tasks related to machine translation. Their CBOW model is also worth noting for its similarities to the composition function used in this paper. Using a slightly different approach, [29], also learned bilingual embeddings for machine translation. It is important to note that, unlike our proposed system, all of these methods require word aligned parallel data for training. 2 Two recent workshop papers deserve mention in this respect. Both Lauly et al. [17] and Sarath Chan- dar et al. [23] propose methods for learning word embeddings by exploiting bilingual data, not unlike the method proposed in this paper. Instead of the noise-contrastive method developed in this paper, both groups of authors make use of autoencoders to encode monolingual representations and to support the bilingual transfer. So far almost all of this work has been focused on learning multilingual representations at the word level. As distributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to also attempt to induce these using cross-lingual data. This paper provides a first step in that direction. 3 Model Description Language acquisition in humans is widely seen as grounded in sensory-motor experience [22, 2]. Based on this idea, there have been some attempts at using multi-modal data for learning better vector representations of words (e.g . [26]). Such methods, however, are not easily scalable across languages or to large amounts of data for which no secondary or tertiary representation might exist. We abstract the underlying principle one step further and attempt to learn semantics from multi- lingual data. The idea is that, given enough parallel data, a shared representation would be forced to capture the common elements between sentences from different languages. What two parallel sentences have in common, of course, is the semantics of those two sentences. Using this data, we propose a novel method for learning vector representations at the word level and beyond. 3.1 Bilingual Signal Exploiting the semantic similarity of parallel sentences across languages, we can define a simple bilingual (and trivially multilingual) error function as follows: Given a compositional sentence model (CV M) MA , which maps a sentence to a vector, we can train a second CVM MB using a corpus CA,B of parallel data from the language pair A, B . For each pair of parallel sentences (a, b) \u2208 CA,B , we attempt to minimize Edist(a, b) =karoot \u2212 brootk 2 (1) where aroot is the vector representing sentence a and broot the vector representing sentence b. 3.2 The BICVM Model A
{"name": "1406.2199v2", "contents": [{"tool_name": "djvu", "text": "Two-Stream Convolutional Networks for Action Recognition in Videos Karen Simonyan Andrew Zisserman Visual Geometry Group, University of Oxford {karen,az}@robots.ox.ac.uk Abstract We investigate architectures of discriminatively trained deep Convolutional Net- works (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion be- tween frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architec- ture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi- task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions bench- marks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification. 1 Introduction Recognition of human actions in videos is a challenging task which has received a significant amount of attention in the research community [11, 14, 17, 26]. Compared to still image classification, the temporal component of videos provides an additional (and important) clue for recognition, as a number of actions can be reliably recognised based on the motion information. Additionally, video provides natural data augmentation (jittering) for single image (video frame) classification. In this work, we aim at extending deep Convolutional Networks (ConvNets) [19], a state-of-the- art still image representation [15], to action recognition in video data. This task has recently been addressed in [14] by using stacked video frames as input to the network, but the results were signif- icantly worse than those of the best hand-crafted shallow representations [20, 26]. We investigate a different architecture based on two separate recognition streams (spatial and temporal), which are then combined by late fusion. The spatial stream performs action recognition from still video frames, whilst the temporal stream is trained to recognise action from motion in the form of dense optical flow. Both streams are implemented as ConvNets. Decoupling the spatial and temporal nets also allows us to exploit the availability of large amounts of annotated image data by pre-training the spatial net on the ImageNet challenge dataset [1]. Our proposed architecture is related to the two-streams hypothesis [9], according to which the human visual cortex contains two pathways: the ventral stream (which performs object recognition) and the dorsal stream (which recognises motion); though we do not investigate this connection any further here. The rest of the paper is organised as follows. In Sect. 1 .1 we review the related work on action recognition using both shallow and deep architectures. In Sect. 2 we introduce the two-stream architecture and specify the Spatial ConvNet. Sect. 3 introduces the Temporal ConvNet and in particular how it generalizes the previous architectures reviewed in Sect. 1 .1 . A mult-task learning framework is developed in Sect. 4 in order to allow effortless combination of training data over 1 a r X i v : 1 4 0 6 . 2 1 9 9 v 2 [ c s . C V ] 1 2 N o v 2 0 1 4 multiple datasets. Implementation details are given in Sect. 5, and the performance is evaluated in Sect. 6 and compared to the state of the art. Our experiments on two challenging datasets (UCF- 101 [24] and HMDB-51 [16]) show that the two recognition streams are complementary, and our deep architecture significantly outperforms that of [14] and is competitive with the state of the art shallow representations [20, 21, 26] in spite of being trained on relatively small datasets. 1.1 Related work Video recognition research has been largely driven by the advances in image recognition methods, which were often adapted and extended to deal with video data. A large family of video action recognition methods is based on shallow high-dimensional encodings of local spatio-temporal fea- tures. For instance, the algorithm of [17] consists in detecting sparse spatio-temporal interest points, which are then described using local spatio-temporal features: Histogram of Oriented Gradients (HOG) [7] and Histogram of Optical Flow (HOF). The features are then encoded into the Bag Of Features (BoF) representation, which is pooled over several spatio-temporal grids (similarly to spa- tial pyramid pooling) and combined with an SVM classifier. In a later work [28], it was shown that dense sampling of local features outperforms sparse interest points. Instead of computing local video features over spatio-temporal cuboids, state-of-the-art shallow video representations [20, 21, 26] make use of dense point trajectories. The approach, first in- troduced in [29], consists in adjusting local descriptor support regions, so that they follow dense trajectories, computed using optical flow. The best performance in the trajectory-based pipeline was achieved by the Motion Boundary Histogram (MBH) [8], which is a gradient-based feature, separately computed on the horizontal and vertical components of optical flow. A combination of several features was shown to further boost the accuracy. Recent improvements of trajectory-based hand-crafted representations include compensation of global (camera) motion [10, 16, 26], and the use of the Fisher vector encoding [22] (in [26]) or its deeper variant [23] (in [21]). There has also been a number of attempts to develop a deep architecture for video recognition. In the majority of these works, the input to the network is a stack of consecutive video frames, so the model is expected to implicitly learn spatio-temporal motion-dependent features in the first layers, which can be a difficult task. In [11], an HMAX architecture for video recognition was proposed with pre-defined spatio-temporal filters in the first layer. Later, it was combined [16] with a spatial HMAX model, thus forming spatial (ventral-like) and temporal (dorsal-like) recognition streams. Unlike our work, however, the streams were implemented as hand-crafted and rather shallow (3- layer) HMAX models. In [4, 18, 25], a convolutional RBM and ISA were used for unsupervised learning of spatio-temporal features, which were then plugged into a discriminative model for action classification. Discriminative end-to-end learning of video ConvNets has been addressed in [12] and, more recently, in [14], who compared several ConvNet architectures for action recognition. Training was carried out on a very large Sports-1M dataset, comprising 1.1M YouTube videos of sports activities. Interestingly, [14] found that a network, operating on individual video frames, performs similarly to the networks, whose input is a stack of frames. This might indicate that the learnt spatio-temporal features do not capture the motion well. The learnt representation, fine- tuned on the UCF-101 dataset, turned out to be 20% less accurate than hand-crafted state-of-the-art trajectory-based representation [20, 27]. Our temporal stream ConvNet operates on multiple-frame dense optical flow, which is typically computed in an energy minimisation framework by solving for a displacement field (typically at multiple image scales). We used a popular method of [2], which formulates the energy based on constancy assumptions for intensity and its gradient, as well as smoothness of the displacement field. Recently, [30] proposed an image patch matching scheme, which is reminiscent of deep ConvNets, but does not incorporate learning. 2 Two-stream architecture for video recognition Video can naturally be decomposed into spatial and temporal components. The spatial part, in the form of individual frame appearance, carries information about scenes and objects depicted in the video. The temporal part, in the form of motion across the frames, conveys the movement of the observer (the camera) and the objects. We devise our video recognition architecture accordingly, dividing it into two streams, as shown in Fig. 1 . Each stream is implemented using a deep ConvNet, softmax scores of which are combined by late fusion. We consider two fusion methods: averaging and training a multi-class linear SVM [6] on stacked L2 -normalised softmax scores as features. 2 conv1 7x7x96 stride 2 norm. pool 2x2 conv2 5x5x256 stride 2 norm. pool 2x2 conv3 3x3x512 stride 1 conv4 3x3x512 stride 1 conv5 3x3x512 stride 1 pool 2x2 full6 4096 dropout full7 2048 dropout softmax conv1 7x7x96 stride 2 norm. pool 2x2 conv2 5x5x256 stride 2 pool 2x2 conv3 3x3x512 stride 1 conv4 3x3x512 stride 1 conv5 3x3x512 stride 1 pool 2x2 full6 4096 dropout full7 2048 dropout softmax Spatial stream ConvNet Temporal stream ConvNet single frame input video multi-frame optical flow class score fusion Figure 1: Two-stream architecture for video classification. Spatial stream ConvNet operates on individual video frames, effectively performing action recog- nition from still images. The static appearance by itself is a useful clue, since some actions are strongly associated with particular objects. In fact, as will be shown in Sect. 6, action classification from still frames (the spatial recognition stream) is fairly competitive on its own. Since a spatial ConvNet is essentially an image classification architecture, we can build upon the recent advances in large-scale image recognition methods [15], and pre-train the network on a large image classifica- tion dataset, such as the ImageNet challenge dataset. The details are presented in Sect. 5 . Next, we describe the temporal stream ConvNet, which exploits motion and significantly improves accuracy. 3 Optical flow ConvNets In this section, we describe a ConvNet model, which forms the temporal recognition stream of our architecture (Sect. 2). Unlike the ConvNet models, reviewed in Sect. 1.1, the input to our model is formed by stacking optical flow displacement fields
{"name": "1412.6334v4", "contents": [{"tool_name": "djvu", "text": "a r X i v : 1 4 1 2 . 6 3 3 4 v 4 [ c s . C L ] 2 2 A u g 2 0 1 5 Published as a conference paper at ICLR 2015 LEVERAGING MONOLINGUAL DATA FOR CROSSLIN- GUAL COMPOSITIONAL WORD REPRESENTATIONS Hubert Soyer National Institute of Informatics, Tokyo, Japan soyer@nii.ac .jp Pontus Stenetorp\u2217 University of Tokyo, Tokyo, Japan pontus@stenetorp.se Akiko Aizawa National Institute of Informatics, Tokyo, Japan aizawa@nii.ac .jp ABSTRACT In this work, we present a novel neural network based architecture for induc- ing compositional crosslingual word representations. Unlike previously proposed methods, our method fulfills the following three criteria; it constrains the word- level representations to be compositional, it is capable of leveraging both bilingual and monolingual data, and it is scalable to large vocabularies and large quantities of data. The key component of our approach is what we refer to as a monolingual inclusion criterion, that exploits the observation that phrases are more closely se- mantically related to their sub-phrases than to other randomly sampled phrases. We evaluate our method on a well-established crosslingual document classifi- cation task and achieve results that are either comparable, or greatly improve upon previous state-of-the-art methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for the English to German and German to English sub-tasks respectively. The former advances the state of the art by 0.9% points of accuracy, the latter is an absolute improvement upon the previous state of the art by 7.7% points of accuracy and an improvement of 33.0% in error reduction. 1 INTRODUCTION Dense vector representations (embeddings) of words and phrases, as opposed to discrete feature templates, have recently allowed for notable advances in the state of the art of Natural Language Processing (NLP) (Socher et al., 2013; Baroni et al., 2014). These representations are typically in- duced from large unannotated corpora by predicting a word given its context (Collobert & Weston, 2008). Unlike discrete feature templates, these representations allow supervised methods to readily make use of unlabeled data, effectively making them semi-supervised (Turian et al., 2010). A recent focus has been on crosslingual, rather than monolingual, representations. Crosslingual representations are induced to represent words, phrases, or documents for more than one language, where the representations are constrained to preserve representational similarity or can be trans- formed between languages (Klementiev et al., 2012; Mikolov et al., 2013b; Hermann & Blunsom, 2014). In particular, crosslingual representations can be helpful for tasks such as translation or to leverage training data in a source language when little or no training data is available for a target language. Examples of such transfer learning tasks are crosslingual sentiment analysis (Wan, 2009) and crosslingual document classification (Klementiev et al., 2012). Mikolov et al. (2013b) induced language-specific word representations, learned a linear mapping be- tween the language-specific representations using bilingual word pairs and evaluated their approach \u2217 Currently at the University College London. 1 Published as a conference paper at ICLR 2015 for single word translation. Klementiev et al. (2012) used automatically aligned sentences and words to constrain word representations across languages based on the number of times a given word in one language was aligned to a word in another language. They also introduced a dataset for crosslingual document classification and evaluated their work on this task. Hermann & Blunsom (2014) intro- duced a method to induce compositional crosslingual word representations from sentence-aligned bilingual corpora. Their method is trained to distinguish the sentence pairs given in a bilingual corpus from randomly generated pairs. The model represents sentences as a function of their word representations, encouraging the word representations to be compositional. Another approach has been to use auto-encoders and bag of words representations of sentences that can easily be applied to jointly leverage both bilingual and monolingual data (Chandar A P et al., 2014). Most recently, Gouws et al. (2014) extended the Skip-Gram model of Mikolov et al. (2013a) to be applicable to bilingual data. Just like the Skip-Gram model they predict a word in its context, but constrain the linear combinations of word representations from aligned sentences to be similar. However, these previous methods all suffer from one or more of three short-comings. Klementiev et al. (2012); Mikolov et al. (2013b); Gouws et al. (2014) all learn their representations using a word-level monolingual objective. This effectively means that compositionality is not en- couraged by the monolingual objective, which may be problematic when composing word repre- sentations for a phrase or document-level task. While the method of Hermann & Blunsom (2014) allows for arbitrary composition functions, they are limited to using sentence-aligned bilingual data and it is not immediately obvious how their method can be extended to make use of monolingual data. Lastly, while the method of Chandar A P et al. (2014) suffers from neither of the above issues, their method represents each sentence as a bag of words vector with the size of the whole vocabulary. This leads to computational scaling issues and necessitates a vocabulary cut-off which may hamper performance for compounding languages such as German. The question that we pose is thus, can a single method 1. Constrain the word-level representations to be compositional. 2. Leverage both monolingual and bilingual data. 3. Scale to large vocabulary sizes without greatly impacting training time. In this work, we propose a neural network based architecture for creating crosslingual compositional word representations. The method is agnostic to the choice of composition function and combines a bilingual training objective with a novel way of training monolingual word representations. This enables us to draw from a plethora of unlabeled monolingual data, while our method is efficient enough to be trained using roughly seven million sentences in about six hours on a single-core desktop computer. We evaluate our method on a well-established document classification task and achieve results for both sub-tasks that are either comparable or greatly improve upon the previous state of the art. For the German to English sub-task our method achieves 84.4% in accuracy, an error reduction of 33.0% in comparison to the previous state of the art. 2 MODEL 2.1 INDUCING CROSSLINGUAL WORD REPRESENTATIONS For any task involving crosslingual word representations we distinguish between two kinds of errors 1. Transfer errors occur due to transferring representations between languages. Ideally, ex- pressions of the same meaning (words, phrases, or documents) should be represented by the same vectors, regardless of the language they are expressed in. The more different these representations are from language 1 (l1) to language 2 (l2), the larger the transfer error. 2. Monolingual errors occur because the word, phrase or document representations within the same language are not expressive enough. For example, in the case of classification this would mean that the representations do not possess enough discriminative power for a classifier to achieve high accuracy. 2 Published as a conference paper at ICLR 2015 The way to attain high performance for any task that involves crosslingual word representations is to keep both transfer errors and monolingual errors to a minimum using representations that are both expressive and constrained crosslingually. 2.2 CREATING REPRESENTATIONS FOR PHRASES AND DOCUMENTS Following the work of Klementiev et al. (2012); Hermann & Blunsom (2014); Gouws et al. (2014) we represent each word as a vector and use separate word representations for each language. Like Hermann & Blunsom (2014), we look up the vector representations for all words of a given sentence in the corresponding lookup table and apply a composition function to transform these word vectors into a sentence representation. To create document representations, we apply the same composition function again, this time to transform the representations of all sentences in a document to a doc- ument representation. For the majority of this work we will make use of the addition composition function, which can be written as the sum of all word representations wi in a given phrase a([w1,w2,\u00b7\u00b7\u00b7 ,wl])= l X i=1 wi (1) To give an example of another possible candidate composition function, we also use the bigram based addition (Bi) composition function, formalized as b([w1,w2,\u00b7\u00b7\u00b7 ,wl])= l X i=2 tanh(wi\u22121 , wi) (2) where the hyperbolic tangent (tanh) is wrapped around every word bigram to produce intermediate results that are then summed up. By introducing a non-linear function the Bi composition is no longer a bag-of-vectors function and takes word order into account. Given that neither of the above composition functions involve any additional parameters, the only parameters of our model are in fact the word representations that are shared globally across all training samples. 2.3 OBJECTIVE Following Klementiev et al. (2012) we split our objective into two sub-objectives, a bilingual objec- tive minimizing the transfer errors and a monolingual objective minimizing the monolingual errors for l1 and l2 . We formalize the loss over the whole training set as Ltotal = Nbi X i=1 Lbi(v l1 i,v l2 i)+ Nmono1 X i=1 Lmono(x l1 i)+ Nmono2 X i=1 Lmono(yl2 i )+\u03bbk\u03b8k2 (3) where Lbi is the bilingual loss for two aligned sentences, vi is a sample from the set of Nbi aligned sentences in language 1 and 2, Lmono is the monolingual loss which we sum over Nmono1 sentences x l1 i from corpora in language 1 and Nmono2 sentences y l2 i from corpora in language 2. We learn the parameters \u03b8, which represent the whole set of word representations for both l1 and l2. The pa- rameters are used in a shared fashion to construct sentence representations for both the monolingual corpora and the parts of the bilingu
{"name": "1502.00873v1", "contents": [{"tool_name": "djvu", "text": "DeepID3: Face Recognition with Very Deep Neural Networks Yi Sun1 Ding Liang2 Xiaogang Wang3,4 Xiaoou Tang1,4 1Department of Information Engineering, The Chinese University of Hong Kong 2 SenseTime Group 3 Department of Electronic Engineering, The Chinese University of Hong Kong 4 Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences sy011@ie.cuhk.edu.hk liangding@sensetime.com xgwang@ee.cuhk.edu.hk xtang@ie.cuhk.edu.hk Abstract The state-of-the-art of face recognition has been signifi- cantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end. 1. Introduction Using deep neural networks to learn effective feature representations has become popular in face recognition [12, 20, 17, 22, 14, 13, 18, 21, 19, 15]. With better deep network architectures and supervisory methods, face recognition accuracy has been boosted rapidly in recent years. In particular, a few noticeable face representation learning techniques are evolved recently. An early effort of learning deep face representation in a supervised way was to employ face verification as the supervisory signal [12], which required classifying a pair of training images as being the same person or not. It greatly reduced the intra-personal variations in the face representation. Then learning discriminative deep face representation through large-scale face identity classification (face identification) was proposed by DeepID [14] and DeepFace [17, 18]. By classifying training images into a large amount of identities, the last hidden layer of deep neural networks would form rich identity-related features. With this technique, deep learning got close to human performance for the first time on tightly cropped face images of the extensively evaluated LFW face verification dataset [6]. However, the learned face representation could also contain significant intra- personal variations. Motivated by both [12] and [14], an approach of learning deep face representation by joint face identification-verification was proposed in DeepID2 [13] and was further improved in DeepID2+ [15]. Adding verification supervisory signals significantly reduced intra- personal variations, leading to another significant improve- ment on face recognition performance. Human face verification accuracy on the entire face images of LFW was surpassed finally [13, 15]. Both GoogLeNet [16] and VGG [10] ranked in the top in general image classification in ILSVRC 2014. This motivates us to investigate whether the superb learning capacity brought by very deep net structures can also benefit face recognition. Although supervised by advanced supervisory signals, the network architectures of DeepID2 and DeepID2+ are much shallower compared to recently proposed high- performance deep neural networks in general object recog- nition such as VGG and GoogLeNet. VGG net stacked multiple convolutional layers together to form complex features. GoogLeNet is more advanced by incorporating multi-scale convolutions and pooling into a single feature extraction layer coined inception [16]. To learn efficiently, it also introduced 1x1 convolutions for feature dimension reduction. In this paper, we propose two deep neural network ar- chitectures, referred to as DeepID3, which are significantly deeper than the previous state-of-the-art DeepID2+ archi- tecture for face recognition. DeepID3 networks are rebuilt from basic elements (i.e., stacked convolution or inception 1 a r X i v : 1 5 0 2 . 0 0 8 7 3 v 1 [ c s . C V ] 3 F e b 2 0 1 5 layers) of VGG net [10] and GoogLeNet [16]. During training, joint face identification-verification supervisory signals [13] are added to the final feature extraction layer as well as a few intermediate layers of each network. In addition, to learn a richer pool of facial features, weights in higher layers of some of DeepID3 networks are unshared. Being trained on the same dataset as DeepID2+, DeepID3 improves the face verification accuracy from 99.47% to 99.53% and rank-1 face identification accuracy from 95.0% to 96.0% on LFW, compared with DeepID2+. The \u201dtrue\u201d face verification accuracy when wrongly labeled face pairs are corrected and a few hard test samples will be further discussed in the end. 2. DeepID3 net For the comparison purpose, we briefly review the previously proposed DeepID2+ net architecture [15]. As illustrated in Fig. 1, DeepID2+ net has three convolutional layers followed by max-pooling (neurons in the third convo- lutional layer share weights in only local regions), followed by one locally-connected layer and one fully-connected layer. Joint identification-verification supervisory signals [13] are added to the last fully-connected layer (from which the final features are extracted for face recognition) as well as a few fully connected layers branched out from intermediate pooling layers to better supervise early feature extraction processes. The proposed DeepID3 net inherits a few characteristics of the DeepID2+ net, including unshared neural weights in the last few feature extraction layers and the way of adding supervisory signals to early layers. However, the DeepID3 net is significantly deeper, with ten to fifteen non-linear feature extraction layers, compared to five in DeepID2+. In particular, we propose two DeepID3 net architectures, referred to as DeepID3 net1 and DeepID3 net2, as illustrated in Fig. 2 and Fig. 3, respectively. The depth of DeepID3 net is due to stacking multiple convolution/inception layers before each pooling layer. Continuous convolution/inception helps to form features with larger receptive fields and more complex nonlinearity while restricting the number of parameters [10]. The proposed DeepID3 net1 takes two continuous con- volutional layers before each pooling layer. Compared to the VGG net proposed in previous literature [10, 19], we add additional supervisory signals in a number of full- connection layers branched out from intermediate layers, which helps to learn better mid-level features and makes optimization of a very deep neural network easier. The top two convolutional layers are replaced by locally connected layers. With unshared parameters, top layers could form more expressive features with a reduced feature dimension. The last locally connected layer of our DeepID3 net1 is used to extract the final features without an additional fully Figure 1: Architecture of DeepID2+ net [15]. Solid arrows show forward-propagation directions. Dashed arrows point the layers on which joint face identification-verification supervisory signals are added. The final feature extraction layer in red box is used for face recognition. connected layer. DeepID3 net2 starts with every two continuous con- volutional layers followed by one pooling layer as does in DeepID3 net1, while taking inception layers [16] in later feature extraction stages: there are three continuous inception layers before the third pooling layer and two inception layers before the fourth pooling layer. Joint identification-verification supervisory signals are added on fully connected layers following each pooling layer. In the proposed two network architectures, rectified linear non-linearity [9] is used for all except pooling layers, and dropout learning [5] is added on the final feature extraction layer. Although with significant depth, our DeepID3 networks are much smaller than VGG net or GoogLeNet proposed in general object recognition due to a restricted number of feature maps in each layer. The proposed DeepID3 nets are trained on the same 25 face regions as DeepID2+ nets [15], with each network taking a particular face region as input. These face regions are selected by feature selection in the previous work [13], which differ in positions, scales, and color channels such that different networks could learn complementary information. After training, these networks are used to extract features from respective face regions. Then an additional Joint Bayesian model [3] is learned on these 2 Figure 2: Architecture of DeepID3 net1. Figure description is the same as Fig. 1. features for face verification or identification. All the DeepID3 networks and Joint Bayesian models are learned on the same approximately 300 thousand training samples as used in DeepID2+ [15], which is a combination of CelebFaces+ [14] and WDRef [3] datasets, and tested on LFW [6]. People in these two training data sets and the LFW test set are mutually exclusive. The face verification performance on LFW of individual DeepID3 net is compared to DeepID2+ net in Fig. 4 on the 25 face regions (with horizontal flipping), respectively. On average, DeepID3 net1 and DeepID3 net2 reduce the error rate by 0.81% and 0.26% compared to DeepID2+ net, respectively. 3. Experiments To reduce redundancy, DeepID3 net1 and net2 are used to extract features on either the original or the horizontally Figure 3: Architecture of DeepID3 net2. Figure description isthe same asFig. 1. Figure 4: LFW face verification accuracy of individual DeepID2+ and DeepID3 net trained on the same face regions in [15]. 3 Table 1: Face verification on LFW. method accuracy (%) High-dim LBP [4] 95.17 \u00b1 1.13 TL Joint Bayesian [2] 96.33 \u00b1 1.08 DeepFace [17] 97.35 \u00b1 0.25 DeepID [14] 97.45 \u00b1 0.26 GaussianFace [7, 8] 98.52 \u00b1 0.66 DeepID2 [13, 11] 99.15 \u00b1 0.13 DeepID2+ [15] 99.47 \u00b1 0.12 DeepID3 99.53 \u00b1 0.10 flipped face region but not both. I
{"name": "1601.02129v2", "contents": [{"tool_name": "djvu", "text": "Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs Zheng Shou, Dongang Wang, and Shih-Fu Chang Columbia University New York, NY, USA {zs2262,dw2648,sc250}@columbia.edu Abstract We address temporal action localization in untrimmed long videos. This is important because videos in real ap- plications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal ac- tion localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as ini- tialization for the localization network; and (3) a localiza- tion network fine-tunes the learned classification network to localize each action instance. We propose a novel loss func- tion for the localization network to explicitly consider tem- poral overlap and achieve high temporal localization accu- racy. In the end, only the proposal network and the local- ization network are used during prediction. On two large- scale benchmarks, our approach achieves significantly su- perior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014. 1. Introduction Impressive progress has been reported in recent litera- ture for action recognition [42, 28, 2, 3, 39, 40, 24, 18, 31, 44, 13, 37]. Besides detecting action in manually trimmed short video, researchers start to develop techniques for de- tecting actions in untrimmed long videos in the wild. This trend motivates another challenging topic - temporal action localization: given a long untrimmed video, \u201cwhen does a specific action start and end?\u201d This problem is important because real applications usually involve long untrimmed videos, which can be highly unconstrained in space and time, and one video can contain multiple action instances plus background scenes or other activities. Localizing ac- tions in long videos, such as those in surveillance, can save tremendous time and computational costs. Most state-of-the-art methods rely on manually selected features, and their performances still require much improve- ment. For example, top performing approaches in THU- MOS Challenge 2014 [27, 41, 17, 15] and 2015 [46, 9] both used improved Dense Trajectory (iDT) with Fisher Vector (FV) [40, 25]. There have been some recent attempts at in- corporating iDT features with appearance features automat- ically extracted by frame-level deep networks [27, 41, 17]. Nevertheless, such 2D ConvNets do not capture motion in- formation, which is important for modeling actions and de- termining their temporal boundaries. As an analogy in still images, object detection recently achieved large improvements by using deep networks. In- spired by Region-based Convolutional Neural Networks (R- CNN) [7] and its upgraded versions [6, 30, 21], we develop Segment-CNN1, which is an effective deep network frame- work for temporal action localization as outlined in Figure 1. We adopt 3D ConvNets [13, 37], which recently has been shown to be promising for capturing motion charac- teristics in videos, and add a new multi-stage framework. First, multi-scale segments are generated as candidates for three deep networks. The proposal network classifies each segment as either action or background in order to eliminate background segment estimated to be unlikely to contain ac- tions of interest. The classification network trains typical one-vs-all classification model for all action categories plus the background. However, the classification network aims at finding key evidences to distinguish different categories, rather than lo- calizing precise action presences in time. Sometimes, the scores from the classification network can be high even when the segment has only a very small overlap with the ground truth instance. This can be detrimental because sub- sequent post-processing steps, such as Non-Maximum Sup- pression (NMS), might remove segment of small score but large overlap with ground truth. To explicitly take tempo- ral overlap into consideration, we introduce the localiza- tion network based on the same architecture, but this net- 1Source code and trained models are available online at https:// github.com/zhengshou/scnn/. a r X i v : 1 6 0 1 . 0 2 1 2 9 v 2 [ c s . C V ] 2 1 A p r 2 0 1 6 Background CliffDiving Input untrimmed video Background (a) Multi-scale segment generation . . . 16 frames 32 frames 64 fr ames ... ... ... generate varied leng th segments (b) Segment-CNN \u00b7 3D ConvNets fc6 fc7 background: action : 0.15 0.85 Segment fc8 Ppro (1) Proposal S o f t m a x L o s s ... ... background: action 1: 0.01 0.02 action 2: . . . acti on K: 0.92 0.01 (2) Classification . . . ... ... . . . background: action 1: 0.02 0.03 action 2: . . . acti on K: 0.82 0.02 Pl oc . . . ... ... . . . (3) Localiza tion temporal overlap class-specific label S o f t m a x L o s s O v e r l a p L o s s 3D ConvNets fc6 fc7 fc8 Pcls 3D ConvNets fc6 fc7 fc8 class-specific label action or not? Segment Segment (c) Post -processing NMS P r e d i c t i o n . . . Cli ffDiving 0.80 \u221a Cli ffDiving 0.75 X . . . Background CliffDiving Background Figure 1. Overview of our framework. (a) Multi-scale segment generation: given an untrimmed video, we generate segments of varied lengths via sliding window; (b) Segment-CNN: the proposal network identifies candidate segments, the classification network trains an action recognition model to serve as initialization for the localization network, and the localization network localizes action instances in time and outputs confidence scores; (c) Post-processing: using the prediction scores from the localization network, we further remove redundancy by NMS to obtain the final results. During training, the classification network is first learned and then used as initialization for the localization network. During prediction, only the proposal and localization networks are used. work uses a novel loss function, which rewards segments with higher temporal overlap with the ground truths, and thus can generate confidence scores more suitable for post- processing. Note that the classification network cannot be replaced by the localization network. We will show later that using the trained classification network (without con- sidering temporal overlap) to initialize the localization net- work (take into account temporal overlap) is important, and achieves better temporal localization accuracies. To summarize, our main contributions are three-fold: (1) To the best of our knowledge, our work is the first to exploit 3D ConvNets with multi-stage processes for tempo- ral action localization in untrimmed long videos in the wild. (2) We introduce an effective multi-stage Segment-CNN framework, to propose candidate segments, recognize ac- tions, and localize temporal boundaries. The proposal net- work improves the efficiency by eliminating unlikely candi- date segments, and the localization network is key to tem- poral localization accuracy boosting. (3) The proposed techniques significantly outperform the state-of-the-art systems over two large-scale benchmarks suitable for temporal action localization. When the overlap threshold used in evaluation is set to 0.5, our approach im- proves mAP on MEXaction2 from 1.7% to 7.4% and mAP on THUMOS 2014 from 15.0% to 19.0%. We did not eval- uate on THUMOS Challenge 2015 [9] because the ground truth is withheld by organizers for future evaluation. More detailed evaluation results are available in Section 4. 2. Related work Temporal action localization. This topic has been studied in two directions. When training data only have video-level category labels but no temporal annotations, researchers formulated this as weakly supervised problems or multi- ple instance learning problems to learn the key evidences in untrimmed videos and temporally localize actions by se- lecting key instances [22, 23]. Sun et al. [36] transferred knowledge from web images to address temporal localiza- tion in untrimmed web videos. Another line of work focuses on learning from data when the temporal boundaries have been annotated for action in- stances in untrimmed videos, such as THUMOS. Most of these works pose this as a classification problem and adopt a temporal sliding window approach, where each window is considered as an action candidate subject to classifica- tion [25]. Surveys about action classification methods can be found in [42, 28, 2, 3]. Recently, two directions lead the state-of-the-art: (1) Wang et al. [39] proposed extract- ing HOG, HOF, MBH features along dense trajectories, and later on they took camera motion into consideration [40]. Further improvement can be achieved by stacking features with multiple time skips [24]. (2) Enlighted by the suc- cess of CNNs in recent works [20, 32], Karpathy et al. [18] evaluated frame-level CNNs on large-scale video classifi- cation tasks. Simonyan and Zisserman [31] designed two- stream CNNs to learn from still image and motion flow re- spectively. In [44], a latent concept descriptor of convo- lutional feature map was proposed, and great results were achieved on event detection with VLAD encoding. To learn spatio-temporal features together, the architecture of 3D ConvNets was explored in [13, 37], achieving competitive results. Oneata et al. [26] proposed approximately normal- ized Fisher Vectors to reduce the high dimensionality of FV. Stoian et al. [35] introduced a two-level cascade to allow fast search for action instances. Instead of precision, these methods focus on improving the efficiency of conventional methods. To specifically address the temporal precision of action detection, Gaidon et al. [4, 5] modeled the struc- ture of action sequence with atomic action units (actoms). The explicit modeling of action units allows for matching more complete action unit sequences, rather than just par- tial content. However, this requires mannual annotations for actoms, which can be subjective and burdensome. Our paper presented her
